\documentclass[twocolumn]{revtex4}
\usepackage{graphics,graphicx,epsfig,amsmath,multirow,gensymb,commath,textcomp,natbib,blindtext,mhchem,tabularx,array,makecell,threeparttable,amssymb,relsize,csquotes}
\usepackage{listings}
\usepackage[a4paper, left=1.85cm, right=1.85cm, top=1.85cm, bottom=1.85cm]{geometry}   
\usepackage[normalem]{ulem}
\newcommand{\squeezeup}{\vspace{-2.5mm}}

\def\bibsection{\section*{\refname}} 
\renewcommand{\thesubsection}{\alph{subsection}}

\renewcommand\theadalign{bc}
\renewcommand\theadfont{\bfseries}
\renewcommand\theadgape{\Gape[4pt]}
\renewcommand\cellgape{\Gape[4pt]}

\begin{document}

\textheight=26.385cm
%Change textheight as the last resort...

\title{Observing Type Ia supernovae, fitting templated light curves, and their usage in cosmology}
 
\author{Jacky Cao, AstroLabs Michaelmas, Lab Partner: Duncan Middlemiss \\ Dates of experiment: 19/10/2017 to 08/12/2017, Date of report: 07/01/2018}

\begin{abstract}              
Type Ia supernovae have the unique trait of being standard candles, their light curves can be used in cosmology to calculate and constrain cosmological parameters. In observing Type Ia supernovae and fitting model light curves to such data one can attempt to derive such values. We have monitored, collected, and analysed data for supernova explosions over a period of 34 days. A $16''$ and a $0.5$ m telescope situated in Durham and La Palma was used for this project. After calculating the magnitudes for a Type Ia (2017hhz) and Type Ia-91bg (2017hle) supernova object, we fitted template light curves with the Python program, \textit{SNooPy}. The quality of fit for the program's \texttt{fit()} function was deemed to be acceptable in accordance to the average reduced $\chi^2$ values calculated for the B and V photometric bands, $\chi^2_{\nu,B} \approx 1.49$ and $\chi^2_{V,\nu} \approx 2.88$ - a good fit requiring $\chi^2_{\nu}=1$. The distance modulus to the supernova 2017hhz was calculated by SNooPy to be $36.121\pm0.106$ mag, using this value we were able to compute $H_0=70\pm20$ km s$^{-1}$ Mpc$^{-1}$. However, the quoted error negates the meaning of the value as it is too uncertain. In improving the accuracy and uncertainties we suggest that more observations of the supernovae were required, and constraining values should be used for the parameters in SNooPy's templates.
\end{abstract}

\maketitle

\vspace{-3ex}
\section{Introduction} 
\label{intro}
\vspace{-2ex}
In astronomy, one of the most violent and luminous events which can occur are supernova explosions. At the end of a massive star's lifetime, as a star runs out of nuclear fuel to burn, there is a possibility that the equilibrium configurations for a star will cease to exist. This leads to a final cataclysmic event, a supernova explosion. The luminosity of such, when at it's peak, can be as bright as a small galaxy \cite{longair}.

Observing these events and measuring their magnitude over a period of time allows us to plot light curves. The likes of which can then be used to visualise the evolution of a supernova. Using these plots we can draw the conclusion that there is order in the explosions. We find that if our sample of supernovae (SNe) is large enough, we see that they can be grouped together into different types just by features from their light curves. The two general types of supernova are Type I and Type II, the difference between them being that Type I lacks the Balmer hydrogen features in it's spectra \cite{longair}, and that Type II events are very hydrogen-rich \cite{obs_phys_class_sn}.

\vspace{-3ex}
\subsection{Supernovae Classification}
\vspace{-2ex}

If we were to further study the light curve shapes and also the spectra of the supernovae from our sample, we would begin to recognise that there are further sub-classifications within Type I and Type II. In Table \ref{table:sn_classes} some of these subtypes are specified.

\begin{table}[h!]
\centering
\begin{tabular}{c@{\hskip 20pt}c} 
 \hline
 \textbf{Type} & \textbf{Characteristic} \\ 
 Ia		& Si II line present at $616.0$ nm in spectra \\
 Ia-91bg	& Absorption trough at $400-450$ nm in spectra \\
 		& \em Type Ib and Type Ic also exist \em \\
 IIP 		& Reaches a plateau in it's light curve  \\
 IIL		& Rapid linear decrease in light curve \\
 		& \em Type IIn and Type IIb also exist \em \\
 \hline
\end{tabular}
\caption{Some of the subclassifications of supernova and their respective characteristics which distinguish between them from each other \cite{longair, obs_phys_class_sn}.}
\label{table:sn_classes}
\end{table}
\squeezeup

In the application to cosmology, one of the most used types of supernovae are Type Ia, their light curve profiles are generally homogenous which allows them to be used as standard candles, thus providing a reliable distance indicator.  

We can see this homogeneity in Figure \ref{fig:typeia}. This plot was produced by superimposing and translating multiple sets of Type Ia data. The light curves in the B and V bands (further discussion on bands and photometric filters in Section \ref{observer-collection}) follow a general path in their evolution: they initially increase in brightness until they reach a peak magnitude, after this point the magnitude of the supernova decreases until no noticeable change can be detected by our telescopes. We note that whilst Type Ia's do peak at different magnitudes in when observed from in different bands, it is not as pronounced as is shown in Figure \ref{fig:typeia}.

\squeezeup
\begin{figure}[!h]
\begin{center}
\includegraphics[width=9.25cm]{intro/typeia}
\caption[]{Plot showing the general forms of Type Ia supernovae in the B and V bands. Archival data was translated along the time axis and magnitude axis to produce the general forms of the light curves. The B band has been offset by a magnitude of $3$ to allow us to see both graphs clearly. The supernovae which were used to plot this graph are: \em{1994ae, 1994S, 1995bd, 1996bo, }\em  and \em{1998aq }\em \cite{jha, matheson}. }
\label{fig:typeia}
\end{center}
\end{figure}
\squeezeup
\squeezeup

In understanding the consistency of Type Ia light curves, we must attempt an exploration of the mechanisms of the explosion. This will involve developing an understanding of the system pre-supernova and also post-explosion.

The general consensus for Type Ia pre-supernova, as discussed by B. W. Carroll and D. A. Ostlie \cite{mod_ast}, is that a white dwarf within a binary system accretes matter from a donor star. However, this is also the initial system for dwarf novae and classical novae, which can grouped together as cataclysmic variables or novae. These too increase in brightness (classical novae by a factor of $\sim 10^6$ \cite{mod_ast}) which can lead to some confusion with supernovae, especially if you observe them optically without following up with any spectroscopic observations. 

It is understood that the difference which leads to cataclysmic variables and Type Ia supernovae is the process of accretion. This is dependent on the binary companion star, if the secondary star is on the main sequence then the white dwarf would just be steadily burning hydrogen or helium in it's surface layers, thus we would have a cataclysmic variable. To produce a supernova explosion we require a progenitor star to increase in mass towards the Chandrasekhar limit of $1.4$ $M_{\odot}$ \cite{posn, longair}. 

If the primary white dwarf was to accrete matter continually from it's binary companion until it itself was brought close to and past the critical mass, then a collapse to a neutron star must occur. However for a Type Ia supernova explosion this is not the case, before the collapse, the star is disrupted by large amounts of energy due to fusion reactions.

We can attribute this energy release to the fusion of carbon and oxygen within the core of the white dwarf. As material is continually accreted and compressed onto the surface, the star is heated and the temperature increases to $T \geq 10^9$ K \cite{longair}. When the thermonuclear energy is released, the white dwarf loses it's electron degeneracy pressure, and so becomes gravitationally unstable \cite{longair}. However, this energy that is produced disrupts the star and prevents the total collapse into a neutron star, this disruption leads to a supernova explosion \cite{posn}. 

In performing spectroscopic observations of Type Ia supernovae at maximum light we see that intermediate mass elements such as silicon, calcium, magnesium, sulphur, and oxygen are present which supports the picture of thermonuclear disruption \cite{longair, mod_ast}.

The process continues until the heaviest element that nuclear fusion can produce is made, iron. This is created from nickel capturing an electron through electron-capture (ec) into cobalt, and then the cobalt captures an electron and $\beta^{+}$ decays into iron \cite{mod_ast}. The process can be summarised in the following equation:
\begin{equation*}
\ce{^{56}Ni} \xrightarrow{\text{ec}} \ce{^{56}Co} \xrightarrow{\text{ec, $\beta^{+}$}} \ce{^{56}Fe}.
\end{equation*}

With these processes, they can be used to explain the form of the Type Ia light curves. The luminosity at peak maximum can be explained as the energy which has been deposited into the expanding envelope from the decay of the \ce{^{56}Ni} nuclei \cite{mod_ast}. We can highlight other features as seen in Figure \ref{fig:typeia}: the increase to peak magnitude is a fast process, it increases roughly half a magnitude a day; and the maximum of the light curve can be approximated as a Gaussian function \cite{mod_ast}. 

However, there are some peculiar types of Type Ia supernovae. For example, as featured in Table \ref{table:sn_classes}, Type Ia-91bg are a class of supernovae which have spectra which resembles those of Type Ia, however, they have an absorption band at $400-450$ nm. This arises due to a blend of Fe-group elements which is dominated by Ti II \cite{obs_phys_class_sn}. With their light curves they also lack the secondary maximum as found in normal Type Ia's within the I and R bands.

Observing objects and recognising that they are Type Ia supernovae thus requires not only spectroscopic observations but also observations in the optical. 

\vspace{-3ex}
\subsection{Supernova Discovery}
\vspace{-2ex}
In attempting to discover supernovae it is important that the search is not confined to a single region of the sky, whilst it may be possible to discover one of these events within that region, it would perhaps take a very long time until you could actually witness and collect data on it. What is thus required is a survey which spans the entire sky. 

A general method which can be employed to discover new supernovae is one which involves data comparison. If a survey takes an image of the entire night sky for every night, then to find new supernovae all that would be required is to subtract one night's data from the previous' and any objects which remain would be objects that could be new supernovae \cite{assasn-rev}. In reality it is not that simple, frames need to be cleaned of noise and they also require to be transformed in response to the point spread function. Plus, follow up observations would have to be made to verify if the object is supernovae or whether it is an anomalous object such as a cataclysmic variable. 

One such survey which employs this is the \em{All-Sky Automated Survey for Supernovae }\em (or ASAS-SN). Built up of five observations stations around the world, they produce a coverage of around $48,000$ square degrees of the night sky and they allow us to see up to a depth of $\sim 17$ mag \cite{asn_lc}, ASAS-SN is able to find many more supernovae and also supernovae which are close to the cores of galaxies, vastly increasing the discovery rate of bright supernova explosions and improving our understanding of where these events occur.

However, the majority of the supernovae are close enough to the detection limit that they need to be validated by other astronomers \cite{asn_lc}. This human review of transients has led to projects such as \textit{Supernova Hunters} \cite{cit-sci}, a citizen science program where volunteers are asked to classify objects which could be potential supernova candidates.  

These programs do not detract from the work that astronomers have to perform in the classification of supernovae, but it is equally important as it allows for the improvement of neural networks so that automatic surveys such as ASAS-SN could work more autonomously and efficiently \cite{cit-sci}.  

\vspace{-3ex}
\subsection{Application to Cosmology} \label{appcosmo}
\vspace{-2ex}
Once optical observations for Type Ia supernovae have been obtained, light curves can then be produced - the conversion of a time series of photometric observations into a set of model parameters for each supernova \cite{sn_consts}, this can then be used for cosmology. 

We find that before Type Ia supernovae data has been corrected for the correlation between maximum luminosity and the width of the light curve about maximum luminosity \cite{longair, abs_phil}, the dispersion of the absolute magnitudes at peak is less than 0.5 magnitudes. Once this correlation is accounted for, there is little dispersion and the light curves lie on top of one another, they are homogeneous in nature. 

With this in mind, if a new Type Ia supernova is discovered and enough data has been collected to produce a light curve, using that and the width-luminosity relation it would be possible to calculate their absolute magnitudes, thus they can be used to determine the redshift-distance relation \cite{mod_ast}.

For example, if we had a sample of nearby Type Ia supernovae with redshifts of $\mathrel{\mathsmaller{\lesssim}} 0.1$, we could derive a value for Hubble's Constant by plotting them on a Hubble diagram \cite{exp_uni_sn}, and in turn use a rearranged form of Hubble's Law,
\begin{equation}
H_0 = \frac{v}{d}, 
\label{eqn:h_0}
\end{equation}
where $H_0$ is the value for Hubble's Constant, $v$ is the recessional velocity of the supernova, and $d$ is the distance to the supernova \cite{mod_ast}. 

To calculate the recessional velocity, the redshift of the supernova has to be obtained, this could be obtained by making spectroscopic observations of the supernovae, and then $v$ would be calculated using,
 \begin{equation}
v = zc,
\label{eqn:redshift}
\end{equation}

where $v$ is the recessional velocity of the supernova, $z$ is the redshift of the supernova, and $c$ is the speed of light in a vacuum \cite{mod_ast}.

Extending the usage of Type Ia's in cosmology, they could also be used to provide statistical constraints on cosmological models, for example for an $\Omega_M$, $\Omega_{\Lambda}$ Universe (where $\Omega_M$ is the mass density which includes ordinary and dark matter, and $\Omega_{\Lambda}$ is the effective mass density of dark energy) \cite{mod_ast, exp_uni_sn}. Additionally, the time-averaged equation of state of dark energy, $w$, could be measured \cite{sn_consts}. This is a value which can be used to characterise the state of the universe - for an accelerating, dark energy dominated universe we would find that $w \approx -1$ \cite{longair}. 

We thus find that Type Ia supernovae are a useful tool which can be applied and used within cosmology.

\vspace{-3ex}
\subsection{Project Aims}
\vspace{-2ex}
This paper discusses the study that was undertaken to observe Type Ia supernovae using ground based telescopes, to learn about the approaches which can be employed to fit template light curves onto collected data, and to evaluate these analytical methods.

In Section \ref{obsver} we describe how we collected and analysed our supernova data, and how we accounted for the uncertainties that arose during the experiment. Our final light curves, and the methods that were used to fit models to the observations is summarised in Section \ref{analysis}. Then in Section \ref{discussion} we further discuss the model fitting, primarily our usage of the fitter program, \textit{SNooPy}, we then discuss the usage of our data for supernova cosmology, and finally we analyse the methods and techniques which we employed to produce our light curves and we explore the potential work that could be performed.

%We also focussed on understanding the sources of uncertainties and attempted to reduce them within our analysis to produce magnitudes which would be more true to what they should be [[?]]. 

%From collected data it would be possible to produce light curves which we can use to identify the type of supernova that we were observing. In also understanding the uncertainties further analysis could be performed such as galaxy subtraction or [[think of something for here...]].

\vspace{-3ex}
\section{Observations} 
% Write in past tense!
\label{obsver}
\vspace{-2ex}
\subsection{Data Collection}
\label{observer-collection}
\vspace{-2ex}
To observe supernova explosions our main pieces of experimental equipment were telescopes which had been fitted with CCD cameras.

We employed three out of the four telescopes based on the Department of Physic's Roof in Durham, and also the robotic telescope on the roof of the William Herschel Telescope building at Roque de los Muchachos Observatory in La Palma.

For the vast majority of our observations we used the 16$''$ telescope (Far-East-16, or FE16) from the Durham array and the robotic 0.5 m telescope (pt5m) from La Palma. We used these two telescopes predominantly as they allowed us the best resolution and illumination on our faint objects. Compared to the 14$''$ telescopes which are also part of the Durham array, having a larger aperture means that a more sufficient number of photons could be obtained so that we could study our targets more clearly. 

Both Far-East-16 and pt5m are variants of the Cassegrain Telescope, they are Ritchey-Chr\'{e}tein Telescopes. This derivative is composed of a hyperboloidal primary and secondary mirror \cite{tel_tech} as can be seen in Figure \ref{fig:cass_spec}. With this design, the telescope can produce good quality images over a field of view of several tens of arcminutes across as opposed to just a few minutes of arc when a basic Cassegrain design is used \cite{tel_tech}. 
\squeezeup
\begin{figure}[!h]
\begin{center}
\includegraphics[width=7.5cm]{observations/cassegrain}
\caption[]{A schematic overview of a Cassegrain/Ritchey-Chr\'{e}tein Telescope, in the former design the primary mirror is paraboloidal, and in the latter the primary mirror is hyperboloidal. In both types of telescope the secondary mirror is a convex hyperboloidal and F is the focal point of the telescope where a CCD camera can be placed. \\ (\textit{This diagram has been adapted from Telescopes and Techniques} \cite{tel_tech}.)}
\label{fig:cass_spec}
\end{center}
\end{figure}
\squeezeup
\squeezeup

To record and make use of the collected light from our supernova targets we had to use CCD cameras which were mounted at the focal points of our telescopes. On Far-East-16, a QSI 583ws CCD was used with an LX200R telescope, and with pt5m a Kodak KAF-3200ME CCD with an Officina Stellare 0.5m Ritchey-Chr\'{e}tein telescope. In a simplified manner, these CCD cameras would convert the photons into electrons and then the electron signal would be processed by a computer into an image \cite{mod_ast, tel_tech}. This could then be manipulated and analysed using software packages such as \textit{GAIA} \cite{starlink}.

For our project in particular, we wanted to produce light curves of supernovae within different colour bands. By using photometric filters on our CCDs we were able to observe supernovae within these bands. Meaning that we were only able to view specific wavelengths of light from our objects, this would correspond to particular energies in the elements present in astronomical bodies \cite{astro_filters}. 

Once we had selected our observation filters and began observations, we had to be acutely aware of the possible over-saturation of the CCDs. As our objects were faint and observing conditions could often be bad, we had to collect as much light as possible before our images became oversaturated due to a brighter object within the same field of view. This would be mitigated by lowering the time that we would be observing for, thus there would be less photons collected from the secondary object.

In spite of this, having just one image of our faint supernovae to analyse would not be representative of it's magnitude. Through the process of image stacking (more in Section \ref{observer-observations}) we were able to improve the quality of the image by compositing together several images of the same area of sky. Thus it was beneficial during our observation nights to take multiple exposures for our chosen given exposure time and also for each photometric filter that we were going to use. 

As we progressed through our experimental period we came to realise that astronomical observations are heavily reliant on weather conditions. What happened to be occurring in the atmosphere that night would have an influence on what you could see with your telescope. This overall effect of the atmosphere on images is called \textit{seeing} \cite{tel_tech}.

In the first instance, having a night sky without any visible clouds would be a good initial step. Once the conditions appear to be clear, the atmospheric turbulence would have to be considered next. This is the process when air is turbulently mixed together at different temperatures, thus creating cells or pockets which have different refractive indexes when compared to their surrounding area \cite{princ_stell_inter}. The effect of this is to produce an image which is blurred and appears to have moved.

In quantifying this seeing as a value, the full width at half maximum (FWHM) can be given for each observation made. The FWHM is a resolution criterion which can be defined as, \textquote{the diameter of the Airy disk at half of its maximum intensity} \cite{princ_stell_inter}. The Airy disk being the central spot of the diffraction pattern which occurs when light from a distant point source has been diffracted by a circular aperture, say, by a telescope. 

Given in arcseconds, the best observing sites produce a stellar image about $0.25''$ across, for an average site it will be typically $2''$ \cite{tel_tech}. What we found was that for Far-East-16, our seeing ranged from $3''$ on a clear night to $\sim6''$ on a cloudy and turbulent night. For pt5m, we found that seeing was generally better on good nights as it is in a better location for astronomical observations. [[why]]

Then to account for atmospheric turbulence, one could use an adaptive optic system to correct for the distortions. For example, an artificial laser guide star could be created and then the amount that it is being perturbed by can be applied to the main object that is being observed \cite{princ_stell_inter}.

However, as Far-East-16 and pt5m are not equipped to perform these measurements, these disturbances could not be accounted for by us. As a result of this, we had to choose nights when the wind was calm, and at times the telescopes had to be refocussed to adapt to the changing atmosphere.

We note that this latter solution was only possible when we were observing in Durham (and not La Palma) as the CCD settings could be manually altered on the fly from the Durham control room.

During our observations bad seeing due to changing weather conditions was not the only factor which would affect us and which we could not account for. Light pollution is a common problem in astronomy and we were no exception to it. With Far-East-16, as the telescope is elevated onto the Department of Physics' roof, the light pollution from the majority of the City of Durham could be avoided. Our problems arose when our objects were in the night sky above Durham Cathedral. This historic building uses spotlights to illuminate parts of itself, some of this light inevitably bleeds off into the sky which creates a halo of light, something not beneficial for magnitude based observations.

Similarly, if our targets were within $40\degree$ or less to the Moon on the celestial sphere, then observations would not be possible as the reflected sunlight would create a disc of light and drown out the supernovae. This effect would especially be magnified if the Moon was in a Full Moon phase.

We thus find that there are multiple factors which we have to be aware of and also consider when performing our observations. From the types of photometric filters to weather conditions, there are multiple factors which contribute to the final data set that we collect.

\vspace{-3ex}
\subsection{Observations Made}
\label{observer-observations}
\vspace{-2ex}
Over a period of 34 days we made observations of 19 different objects, some Type Ia supernovae, a few Type II, and some cataclysmic variables. These objects are noted in our objects log in Appendix \ref{app:objects_log}, and our individual observations from the entire experiment are detailed in Appendix \ref{app:observations_log}.

It was realised early on in our project that it would not feasible for us to track multiple objects for an extended period. This was firstly because observation time had to be distributed equally amongst all the projects within AstroLabs, it would be unfair to observe just supernovae for the entire night; secondly with such a large amount of data, we would not be able to process and analyse it all; and thirdly, some of the targets were just not worth observing at all. These objects would be too close to their host galaxy, or they would be too low on the horizon for the telescopes in Durham or La Palma to view them.

In choosing which objects to observe we used the \textit{Rochester Bright Supernova} \cite{rochester_sn} and \textit{ASAS-SN Supernovae} \cite{asassn_sn} Lists. These are databases which are updated by astronomers with the latest discovered supernovae. For our observations we aimed to choose supernovae which had a discovery magnitude of around 18 or less, anything greater than this would be too faint to observe with our telescopes.

After reviewing and considering which objects would provide the best basis for producing Type Ia light curves, we chose two supernovae targets: 2017hhz, a Type Ia in galaxy UGC 01218; and 2017hle a Type Ia-91bg in NGC 0387. Both being not too close to their host galaxy's nucleus and they appeared to the brightest supernova objects discovered at the time.

However, they were still faint objects so we had to vary the exposure time between 60s and 120s, and often take more than 2 exposures for each band. In doing so we would later be able to improve the quality and brightness of the supernovae seen in the image through noise reduction and image stacking.

\vspace{-3ex}
\subsection{Data Analysis} \label{data_analysis}
\vspace{-2ex}
As data was continually collected over the experimental period, we had to simultaneously analyse it to ensure that the magnitudes we were obtaining would fit supernova light curves.

Photometry was thus performed on each of our frames. Our goals were to: take our supernova images, reduce unwanted noise, and then obtain a magnitude value with a related uncertainty. After this, we could use these values in template fittings, and then attempt to perform cosmology.

Taking our raw frames we had to reduce the noise from unwanted sources. Two sources which we did not have to consider directly was the bias and dark current. The bias arises due to the readout of a CCD, it creates noise when it is converting the captured signal from analogue to digital \cite{obs_uni}. To account for this, bias frames of zero second exposures are taken and subtracted from the data before the data is stored. On the other hand, dark current is the creation of electrons due to the thermal energy of the CCD, these electrons would be read as a signal \cite{obs_uni}. However, the chips are normally cooled to about $-10\degree$ to prevent any such noise from forming.

The CCD produces one other type of noise which can be account for, each pixel on the CCD varies slightly from each other in terms of sensitivity, something which arises due to the manufacturing process. This sensitivity is normally removed by producing frames of a uniformly lit area, flat-fields. The twilight sky or the inside of the telescope dome can be imaged, and this can then be subtracted from our data.
 
Cosmic rays which register onto the camera can be removed by stacking (combining) multiple frames of the same field and same exposure time together. This is a process where the median value for each pixel is found from the stack of images, thus any anomalously high values which arise from cosmic rays can be removed.

Once our frames had been cleaned of noise we could then begin to perform photometry, more specifically, aperture photometry. After identifying the supernova in our images, we then wanted to calculate the apparent magnitude of it and see how it progressed with time. To perform this we had to choose at least two calibration stars, of which they had to be of similar size to the supernova and be in the general proximity of it.  This was so that when performing aperture photometry a similar signal could be obtained across all objects and a similar sky background noise could be obtained as well. Noise which arose from the night sky and which could not be directly removed through subtraction techniques.

Another important criterion for these calibration objects is that their apparent magnitudes must already be known. This was required in the calculation of the magnitudes for the supernova. So, we used the \textit{UCAC4 Catalog} to find the photometric data for the calibration stars in the B and V bands. For the 2017hhz frames we chose calibration stars 512-002598 and 512-002599, and for 2017hle we used 613-003374 and 613-003367. These names are the identifiers used by UCAC4. Once we had our data and calibration stars, we could use the following equation, 
\begin{equation}
    m = z - 2.5 \log_{10}{C},
\label{eqn:mag_counts}
\end{equation}

where $m$ is the magnitude of an object, $z$ is the zero-point of the frame, and $C$ is the number of counts we receive from the object.

In using this equation we first required the zero-point for the image that we were working with. What we imaged would not be the actual magnitudes of the objects, they would be offset by a certain amount, $z$, due to light being absorbed by dust in the supernova's host galaxy, through the interstellar medium, and by the dust that is in our own galaxy.

To begin with then, the counts $C$ for the calibration stars were measured, with this and their apparent magnitudes $m$ we could solve equation \ref{eqn:mag_counts} for $z$. Once this was done for each star within each required band, the $z$ values could then be all averaged together within their respective observation bands. This zero-point could then be used with the number of counts for the supernova to produce an apparent magnitude value for the supernova.

With obtaining a value for the counts, whether from the supernovae or from the calibration objects, we had to choose a fixed aperture size to count the signal that was contained within in, and a fixed sky annulus to count the sky background noise. It was important that the radius of both the aperture and annulus would maximise the signal whilst reducing the amount of sky noise measured.

This was done by selecting one of the calibration stars and then plotting how the signal-to-noise ($S/N$) ratio would change as the radius was increased. Derived from High Energy Astrophysics \cite{longair}, we used the following equation for the signal to noise,
\begin{equation}
\frac{S}{N} = \frac{N_{\gamma}}{\sqrt{N_{\gamma}+N_{b}+N_{d}+R^2}},
\end{equation}

where $N_{\gamma}$ is the supernova signal, $N_{b}$ the sky background noise, $N_{d}$ the noise due to dark current, and $R$ the readout noise. The latter two sources of noise were reduced and could be ignored in our calculations.

In Figure \ref{fig:2017_stn}(a) we show the $S/N$ plot that was created for the 2017hhz frames, the calibration star (512-002599) was selected from the V band data from the 20th October 2017 as it appeared to have the least noise.
\squeezeup
\begin{figure}[!h]
\begin{center}
\includegraphics[width=8.5cm]{observations/2017hhz}
\caption[]{(a) Plot of signal to noise against the radius of the aperture for 2017hhz frames, this was used to identify which fixed radius to use. (b) Image of the aperture tool used: inner white circle around star is the main aperture, the outer yellow annuli is used to measure the sky background. Object shown is the star 512-002599.}
\label{fig:2017_stn}
\end{center}
\end{figure}
\squeezeup
\squeezeup

The point where the plot decreases was chosen to be the radius as this would produce an optimal value for the signal to noise. In the case for 2017hhz this was chosen to be 4 pixels and for 2017hle it was 3 pixels. 

Figure \ref{fig:2017_stn}(b) shows the aperture tool that was used extensively for the aperture photometry. With the white circle as the main aperture that we varied, and the two outer yellow circles being the sky annuli. The area enclosed in the sky would be sampled and then as quantified amount of noise, it would be removed away from the main counts value. The size of these rings were chosen as the radius scaled by factors $\times 1.5$ and $\times 2$. These numbers were justified as we wanted to avoid taking other stars as part of the sky background, and whilst this did mean we had to take part of the galaxy when we were working with our supernovae, we chose to be consistent with our calibration stars as it was difficult to balance just having sky in each individual annuli.

For the placements of these apertures, we took a histogram profile of the vertical and horizontal directions of the object, and then placed the centre of the aperture to the centre of the maximum brightness. 

After we had calculated magnitude values we could then proceed to plot supernova light curves with our data, the process of which can be found in Section \ref{supernovae_models}.

\vspace{-3ex}
\subsection{Data Uncertainties}
\vspace{-2ex}
Throughout the experiment it was important that we kept in mind the uncertainties which could affect the accuracy of our results. Various steps were undertaken to ensure that the sources of errors could be reduced, some of these methods have already been detailed in Section \ref{data_analysis} however we would like to discuss other random and systematic uncertainties.

In collecting data, we have already learnt how a CCD could count extra noise in the form of bias and dark current if it is not accounted for. Alternatively, we found that as our Type Ia supernovae progressed along their evolutionary path we found that as they continually dimmed, it became increasingly difficult for us to produce a precise result. This random error is a result of the observational limits of the telescopes, as they could only observe to a certain magnitude it would be expected that the magnitudes produced would have a larger uncertainty.

Having access to a larger telescope, one with a greater aperture would allow us to collect more light and thus produce a more accurate and precise value for the supernova magnitudes.

If we consider the systematic errors of our experiment, the greatest source would be as a result of our aperture photometry. Whilst we tried to ensure that we used a set method and set conditions, we may still have accidentally misplaced the positioning of one of the apertures. 

We chose to perform the photometry manually, however to reduce the potential systematic error, we should have automated the process with a script. In doing so, we would be more certain that the counts and magnitudes that we obtained would be more accurate to the true value of the magnitude of the supernova.

On the other hand, we could have repeated the photometry and then took an average value for the counts to reduce any random and systematic errors which may occur. However, as we tried to ensure that our apertures were on the position of peak brightness of their respective objects, hopefully it would make no difference. 

\vspace{-3ex}
\subsection{Final Data}
\vspace{-2ex}
Tables \ref{table:2017hhz_data} and \ref{table:2017hle_data} contain the calculated magnitudes with their associated uncertainties. We note again that Equation \ref{eqn:mag_counts} was used for the magnitude, and Equation \ref{m_uncert} for the uncertainty. From this we could then plot Type Ia supernova light curves and then attempt to perform cosmology with the results.

\begin{table}[h!]
\centering
\begin{tabular}{c@{\hskip 20pt}c@{\hskip 20pt}c@{\hskip 20pt}c@{\hskip 20pt}c} 
 \hline
 \textbf{Date Observed} & \textbf{$\boldsymbol{m_B}$} & \textbf{$\boldsymbol{\Delta{m_B}}$} & \textbf{$\boldsymbol{m_V}$} & \textbf{$\boldsymbol{\Delta{m_V}}$} \\ [0.5ex] 
 20/10/17 & 16.8 & 0.1 & 17.1 & 0.1 \\
 22/10/17 & 16.9 & 0.1 & 16.8 & 0.1 \\
 23/10/17 & 16.8 & 0.2 & 16.9 & 0.1 \\
 26/10/17 & 17.0 & 0.2 & 17.0 & 0.1 \\
 29/10/17 & 17.2 & 0.1 & 17.0 & 0.1 \\
 30/10/17 & 17.3 & 0.1 & 17.1 & 0.1 \\
 05/11/17 & 17.7 & 0.2 & 17.3 & 0.2 \\
 11/11/17 & 18.2 & 0.4 & 17.5 & 0.2 \\
 16/11/17 & 18.6 & 0.2 & 17.8 & 0.1 \\
 17/11/17 & 18.8 & 0.2 & 18.2 & 0.4 \\
 \hline
\end{tabular}
\caption{10 days worth of data was analysed to produce magnitudes and their uncertainties for the B and V photometric bands for Type Ia supernova, 2017hhz. }
\vspace{-0.5em}
\label{table:2017hhz_data}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{c@{\hskip 20pt}c@{\hskip 20pt}c@{\hskip 20pt}c@{\hskip 20pt}c} 
 \hline
 \textbf{Date Observed} & \textbf{$\boldsymbol{m_B}$} & \textbf{$\boldsymbol{\Delta{m_B}}$} & \textbf{$\boldsymbol{m_V}$} & \textbf{$\boldsymbol{\Delta{m_V}}$} \\ [0.5ex] 
 26/10/17 & 17.8 & 0.3 & 16.8 & 0.1 \\
 29/10/17 & 18.4 & 0.2 & 17.0 & 0.1 \\
 30/10/17 & 18.6 & 0.2 & 17.1 & 0.1 \\
 16/11/17 & 19.5 & 0.3 & 18.9 & 0.2 \\
 17/11/17 & 19.3 & 0.4 & 18.7 & 0.3 \\
 18/11/17 & 19.6 & 0.5 & 18.7 & 0.3 \\
 \hline
\end{tabular}
\caption{6 days worth of data was analysed for the Type Ia-91bg supernova, 2017hle. For the B and V band, their magnitudes are shown with their associated uncertainties.}
\vspace{-1.5em}
\label{table:2017hle_data}
\end{table}

\vspace{-3ex}
\section{Analysis}
\label{analysis}
\vspace{-2ex}
\subsection{Supernovae Models}
\label{supernovae_models}
\vspace{-2ex}
In fitting templates and models to our data, the process could be manually performed, however we chose to use a Python package \textit{SNooPy} \cite{car_snoopy}. SNooPy is an automated fitter for Type Ia light curves, our magnitudes would be parsed and then a template would be transformed to fit that set of data. 

For the basic fitting function, \texttt{snpy.fit()}, it would only work if we provided a set of data in at least two different photometric bands out of the passbands of uBVgriYJHK. SNooPY also required a right ascension (RA) and declination (dec) of the supernova in order for it calculate the galactic extinction from Schlegel maps \cite{snoopy_docs}. On top of that, a value for the heliocentric redshift of the supernova was also needed for the fitting, this is because it is required for the computation of the K-corrections in each of the filters provided \cite{car_snoopy}.

The redshift values for our supernovae were obtained from the discovery notes that were attached to their entry's on the Rochester Bright Supernova List. As our objects were Type Ia they generally would be of interest for cosmology, spectroscopic observations would thus often be performed after their initial discovery, and so we could use the redshift that they obtained.

For 2017hhz we used a redshift of $0.0392$, and for 2017hle $0.015$ was used.

After SNooPy had managed to apply the template light curves, this fitting could then be exported and plotted. In addition to this, the final parameters that had been selected by SNooPy could be obtained as well. These included the host galaxy extinction (EBVhost), the time for B-maximum (Tmax), the distance modulus to the supernova (DM), and the decline-rate parameter (dm15). Values which would be useful to us when we attempted to perform some cosmology.

To see the correlations between these four parameters, covariance plots could be produced using the \texttt{fitMCMC()} function with the argument \texttt{plot\_triangle} selected as \texttt{True}. A covariance matrix with the values could be retrieved and then potentially plotted to produce a heat map as an alternative to the triangle plots. 

A full example of how SNooPy was used has been provided in Appendix \ref{app:using_snoopy}.

\vspace{-3ex}
\subsection{Results}
\vspace{-2ex}
After running our data through SNooPy, we were able to produce Type Ia light curves with the \texttt{fit()} function for 2017hhz and 2017hle. In Figure \ref{fig:2017hhz_lc} and Figure \ref{fig:2017hle_lc} we show the respective light curves, their plotted magnitudes and their uncertainties for the B and V bands. Table \ref{table:snpy_summary} provides a summary of the parameters that was used by SNooPy for the \texttt{fit()} fitting.

In Table \ref{table:chi2} the $\chi^2$ and reduced $\chi^2$ statistics have been calculated for both supernovae, and for both B and V bands.

Figure \ref{fig:covar_plots} shows some of the covariance plots produced by \texttt{fitMCMC()} for 2017hhz in SNooPy displaying the correlation between the time for B-maximum, the host galaxy extinction, the distance modulus, and the decline-rate parameter. Table \ref{table:snpy_mcmc_summary} shows the parameter output for the fitting made with this function.

With Figure \ref{fig:2017hhz_collage} two samples of our data, a frame of our analysed data, and one of our extension projects are shown together as a collage of images.

\begin{figure}[!h]
\begin{center}
\includegraphics[width=9.25cm]{results/2017hhz}
\caption[]{Plot showing the light curves fitted by SNooPy for supernova 2017hhz. The raw magnitude data has been plotted with their uncertainty in the B and V photometric bands - the V band has been offset by 1 to more clearly show both sets of data. The zero position on the $x$- axis represents the first observation made on Friday 20th October 2017.}
\vspace{-1.5em}
\label{fig:2017hhz_lc}
\end{center}
\end{figure}

\begin{figure}[!h]
\begin{center}
\includegraphics[width=9.25cm]{results/2017hle}
\caption[]{The calculated magnitude and uncertainties for the supernova 2017hle has been plotted along with the SNooPy fitted light curve for the B and V bands. The set of data for the V band has been offset by 1 to produce a clearer plot. Thursday 26th October 2017 represents the zero point on our $x$- axis.}
\label{fig:2017hle_lc}
\vspace{-1.5em}
\end{center}
\end{figure}

\begin{table}[h!]
\centering
\begin{tabular}{c@{\hskip 15pt}c@{\hskip 15pt}c@{\hskip 15pt}c@{\hskip 15pt}c} 
 \hline
 \textbf{Supernova} & \textbf{EBV} & \textbf{T$_{\boldsymbol{\max}}$} & \textbf{DM} & \textbf{$\boldsymbol{\Delta m_{15}}$} \\ [0.5ex] 
 2017hhz & 0.049 & 2.080 &  36.121 & 0.798 \\
  & $\pm$0.083 & $\pm$1.158 & $\pm$0.106 & $\pm$0.138 \\
  
 2017hle & 0.614 & -3.976 &  34.096 & 1.929 \\
  & $\pm$0.225 & $\pm$1.538 & $\pm$0.455 & $\pm$0.238 \\
 \hline
\end{tabular}
\caption{The parameters that were used by \texttt{fit()} in the fittings of light curves for 2017hhz and 2017hle. Where EBV is the host galaxy extinction, T$_{\max}$ is the time for B-maximum since our observations began, DM is the distance modulus, and $\Delta m_{15}$ is the decline rate parameter. The values and their systematic uncertainties have been quoted in full as they were used in their entirety in SNooPy's fitting. }
\vspace{-1.5em}
\label{table:snpy_summary}
\end{table}

\begin{figure}[!h]
\begin{center}
\includegraphics[width=8.25cm]{results/covariance/covariance}
\caption[]{Four out of the six covariance plots produced by the \texttt{fitMCMC()} function in SNooPy for supernova 2017hhz. The plots show the correlations between four parameters: Tmax, the time for B-maximum; EBVhost, the host galaxy extinction; DM, the distance modulus; and dm15, the decline-rate parameter.}
\label{fig:covar_plots}
\vspace{-1.5em}
\end{center}
\end{figure}

\begin{table}[h!]
\centering
\begin{tabular}{c@{\hskip 15pt}c@{\hskip 15pt}c@{\hskip 15pt}c@{\hskip 15pt}c} 
 \hline
 \textbf{Supernova} & \textbf{EBV} & \textbf{T$_{\boldsymbol{\max}}$} & \textbf{DM} & \textbf{$\boldsymbol{\Delta m_{15}}$} \\ [0.5ex] 
 2017hhz & 0.047 & 2.250 &  36.096 & 0.815 \\
  & $\pm$0.067 & $\pm$0.905 & $\pm$0.113 & $\pm$0.076 \\
 \hline
\end{tabular}
\caption{Output of the parameters used for \texttt{fitMCMC()} fitting. Where EBV is the host galaxy extinction, T$_{\max}$ is the time for B-maximum since our observations began, DM is the distance modulus, and $\Delta m_{15}$ is the decline rate parameter. Values and their systematic uncertainties have been quoted in their entirety as used by SNooPy in the fitting. }
\label{table:snpy_mcmc_summary}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{c@{\hskip 20pt}c@{\hskip 20pt}c@{\hskip 20pt}c@{\hskip 20pt}c} 
 \hline
 \textbf{Supernova} & \textbf{$\boldsymbol{\chi^2_B}$} & \textbf{$\boldsymbol{\chi^2_{\nu,B}}$} & \textbf{$\boldsymbol{\chi^2_V}$} & \textbf{$\boldsymbol{\chi^2_{\nu,V}}$} \\ [0.5ex] 
 2017hhz & 5.03 & 1.68 & 8.23 & 2.74 \\
 2017hle & 3.86 & 1.29 & 9.05 & 3.02 \\
 Average & - & 1.49 & - & 2.88 \\
 \hline
\end{tabular}
\caption{$\chi^2$ and reduced $\chi^2$ ($\chi^2_{\nu}$) values are shown for each photometric filter used in our observations (B,V) for each of our supernovae, 2017hhz and 2017hle.}
\label{table:chi2}
\end{table}

\begin{figure}[!h]
\begin{center}
\includegraphics[width=8.25cm]{images/collage_rectangle_small}
\caption[]{Images of the supernova 2017hhz, it's host galaxy, and the stars in the same field in the V band. (a) Our first frame for this supernova from the 20th October 2017. Supernova 2017hhz has been labelled as SNe, and our calibration stars, 512-002598 as 1, and 512-002599 as 2. (b) Our last frame of data taken on 23rd November 2017. (c) Data from 23rd November 2017 when flat-fielded. (d) An attempt was made as an extension project to remove the background galaxy for the data from 20th October 2017, supernova 2017hhz has been labelled for your convenience.}
\label{fig:2017hhz_collage}
\vspace{-1.5em}
\end{center}
\end{figure}

\vspace{-3ex}
\section{Discussion}
\label{discussion}
\vspace{-2ex}

\subsection{SNooPy, the Type Ia SNe template fitter}
\vspace{-2ex}
\subsubsection{What is the initial quality of the fit?}
\vspace{-2ex}
When we look at the plots that SNooPy produced using \texttt{fit()}, in Figures \ref{fig:2017hhz_lc} and \ref{fig:2017hle_lc} we can initially judge by eye that it appears to be a good fitting, the majority of the points seem to lie on the line with their uncertainties. However if we take a moment to step back and consider Figure \ref{fig:2017hle_lc}, we see that whilst SNooPy has managed to fit a template to our Type Ia-91bg data points in both of our required bands, the latter three points in the B band do not appear to have been well fitted at all.

This then raises the questions, how does this program actually fit templates to data, and how well does it do so? Could we believe and use their results in cosmology?

Before we consider the inner workings of SNooPy, we must look at the possibility that the data we collected for those three days was just poor, and this appears to be the most likely case. With these points, the data was collected around two weeks after our initial observations, the supernova would have more than likely dimmed past the limit to which our telescopes could see. Therefore what we had collected would not have been an accurate magnitude, and this does show with the template fitting.

Returning to our initial discussion on SNooPy's ability to fit templates, for us to compare and contrast different results we will use the value for the peak intrinsic magnitude in the B band $m_B$, and the time for this maximum $t_{\max}$. With this we will introduce Philips' parameter, $\Delta m_{15}(B)$, the mean rate of decline of the B light curve between maximum light and a time interval of 15 days after peak \cite{abs_phil}. Using this value which we can obtain from SNooPy we can then use the following equation to calculate the peak B band magnitude,
\begin{equation}
M_{\max}(B) = -21.726 + 2.698 \Delta m_{15}(B),
\label{intrinsic_b}
\end{equation}

where $\Delta m_{15}(B)$ is the decline rate parameter \cite{high_en_astro}.

Using Equation \ref{intrinsic_b} and the values from Table \ref{table:snpy_summary}, for our final fittings we find an absolute maximum magnitude of $-19.6\pm0.3$ for 2017hhz, and $-16.5\pm0.6$ for 2017hle. As a comparison we have calculated this with the \texttt{fitMCMC()} data from Table \ref{table:snpy_mcmc_summary}, $-19.5\pm0.2$. The typical absolute B band magnitudes for Type Ia supernovae is $M_B = -19.5 \pm 0.1$ \cite{longair}, which both our 2017hhz values agrees with. Plus for our Type Ia-91bg, Phillips (1993) \cite{abs_phil} finds a value of $M_B = -16.38\pm0.2$ which is also in agreement. 

From this, we find that with our data and with SNooPy's \texttt{fit()} fitting we produce absolute magnitude values which lie within the typical found values. Yet we still have to further question the quality of this fit. Earlier in the project, when we did not have the latter three values for 2017hle, we ran the SNooPy routine and found a value $\Delta m_{15}=0.073\pm0.039$ which produces $M_B = -21.5\pm0.1$. Comparing this to our full data set value and Philips' value we see that it is not an accurate value whatsoever. 

We can thus draw our first conclusion about SNooPy (and about supernova template fitting in general), a complete set of data is required to effectively produce a light curve. Data which covers the majority of the evolution of the Type Ia supernova - from before peak magnitude to afterwards when it is in decline. 

Removing the latter three points for 2017hhz and running SNooPy we find, $\Delta m_{15}=1.036\pm0.243$ and thus $M_B = -18.9\pm0.7$. This is more inline with what is typical, we could say this is because we still have seven points which form the general profile of the light curve, thus a fitting would be more likely to be accurate.

But for our final plots, we still have to remain skeptical. The positioning of the peak magnitude in the B and V bands is the main contending issue, whilst it appears for 2017hhz we capture our initial data when it is at it's peak magnitude, we cannot truly be sure if this is the case. We require more data before our first point on the 20th October 2017. 

We did attempt to add additional points by looking at the initial observations performed by other astronomers. However, their measurements were made in different photometric bands, and we were unsure on how to convert between different systems, thus we did not include them. 

If we return to our discussion on the quality of the fit, numerically, we can quantify this with $\chi^2$ statistics. It has been defined that a model is a good fit to the data if $\chi^2_{\nu} \approx 1$ (the reduced $\chi^2$) \cite{hugheshase}. From Table \ref{table:chi2} we see that for the B photometric band, both 2017hhz and 2017hle have better fits than their counterparts in the V. Whilst this is a test of the fit of the model to the data, we once again cannot detract from the possibility that the data we collected and analysed was inaccurate due to observation limits or due to the frames not being entirely clean of noise. So, the values produced would be far from unity. 

Since we began this section we have looked at the results which SNooPy has created and have drawn basic conclusions on the quality of fit by calculating the absolute magnitudes of the supernovae and the $\chi^2_{\nu}$ values for the model. We have not yet explored the implications of the results from \texttt{fitMCMC()}, but we would like to first understand the underlying methodology that has been employed by the program, by extension we could then apply this in our own supernova light curve fitting routine. 

\vspace{-2ex}
\subsubsection{How is the fitting actually performed?}
\vspace{-2ex}

In fitting templates there are many well-established methods which can be employed to fit data in a broad number of photometric bands. We must start though at the beginning, and understand the very basics behind fitting Type Ia light curves.

Within the B and to an extent V-band, light curves can generally be fitted by using a single template which has been stretched along the time axis to fit the observed data \cite{highred_perl}. However, applying this to longer wavelengths would be more difficult as other features would be present, for example, an inflection observed in the r band after 20 days will evolve into a secondary maximum when viewed in the i band \cite{car_snoopy}. More templates have to be derived and produced for observations made in other photometric filters.

So, a single-parameter family of curves became the accepted base method instead of using the stretch-like correction. This single parameter would be used to describe the shape of the light curve and would be employed when the template is being fitted.

One package which we have not used, SALT2 \cite{salt2}, uses a stretch-like parameter $x_1$ to describe a sequence of spectral energy distributions of Type Ia supernovae which is further modified by a colour-like parameter $c$. Another fitter on the other hand uses a luminosity parameter $\Delta$, MLCS2k2 \cite{prec_indic, mlcs2k2}. 

With SNooPy none of these parameters are used, instead, the decline-rate parameter ($\Delta m_{15}(B)$) is employed in the making of SNooPy's templates. Compared to the other two methods, the advantage that is given for this is that it is a characteristic of the observed light curve which can be easily measured \cite{car_snoopy}. But as it is tied to a particular filter and photometric system, it cannot be not universal.

Detailed by Burns et al (2011) \cite{car_snoopy}, the initial theory and methods which build upon $\Delta m_{15}$ are beyond the scope of our knowledge and the purpose of this experiment, they cover topics such as: spline fittings, the interpolation of a sample four-dimensional surface, and the fitting of bi-variate polynomial functions. 

What is actually useful for us to know is that after the creators of SNooPy had created some basic light curve templates, they arrived at the following conclusion:

\textquote{If we use the above procedure to generate a B light-curve template with a particular $\Delta m_{15}$, there is no guarantee that the \textit{measured} change in magnitude of the template from peak to day 15 will exactly match the input $\Delta m_{15}(B)$.} \cite{car_snoopy}.

In other words, for the values of the decline-rate parameter which could be obtained after it had performed it's fitting, they do not correspond to the values which were used in the production of the templates. So to use these templates uncorrected for, inaccuracies would certainly arise if we were to use these $\Delta m_{15}$ values for calculating $M_{\max}(B)$. 

Building upon this, we noted earlier that it was difficult to accept a light curve which had been fitted for a few data points from the period when the supernova is in decline. It was recognised by the creators of SNooPy that with a single parameter family of curves, reproducing observations for templates away from maximum light is currently inadequate \cite{car_snoopy}. They suggest that at least another parameter should be introduced in the future to improve the fittings.  

In the mean time, they have just included the intrinsic variations as an extra error in the template, and as  the fitting of their templates incorporate contributions from every photometric band provided, there is an extra random and systematic uncertainty.

Whilst the decline rate parameter is main parameter used in the producing of light curves, fitting them requires the  \texttt{fit()} and \texttt{fitMCMC()} functions to find the best fit parameters for not just $\Delta m_{15}$, but for the host galaxy extinction EBVhost, time for B-maximum (Tmax), and the distance modulus (DM) as well. The uncertainties quoted with all four of these value have a statistical component as a result of their fittings \cite{car_snoopy}.  

\texttt{fit()} uses a non-linear least-squares algorithm in it's fitting, Levenberg-Marquardt. It minimises the variance-weighted residuals of the data from the model \cite{snoopy_online_docs} - it attempts to find the minimum $\chi^2$. 

\texttt{fitMCMC()} on the other hand uses a Markov Chain Monte Carlo (MCMC) approach which provides an advantage of a less biased estimator of the regression parameters, and it can use more sophisticated models of the statistical processes that produced the data \cite{car_snoopy}. 

Considering all of this, we find that the main tool used by SNooPy in the fitting of supernova templates is statistical analysis but with considerations for other factors which compose a template. If we had performed these fittings manually, we most likely would have just performed a stretching of an averaged Type Ia light curve onto our data and calculated $\chi^2$ values to see if it would be a good fit or not.

We now arrive at the point where we have some understanding of how SNooPy and template fitters work, we shall therefore return to our earlier discussion on the quality of the fitting. 

\vspace{-3ex}
\subsubsection{So, how do we view the quality of the fit now?}
\vspace{-2ex}

As we chose to use \texttt{fit()} as our main tool, we did sacrifice some potential accuracy by not using the MCMC based function. When comparing Table \ref{table:snpy_summary} and \ref{table:snpy_mcmc_summary}, we see that the parameters differ from each other in a range from $\sim0.06\%$ to $\sim8\%$. Whilst this is not orders of magnitude different, if we were to use our results in cosmology we would use the summary output from \texttt{fitMCMC()} as it uses a less biased estimator of the parameters.

Viewing the covariance plots for this function, from Figure \ref{fig:covar_plots} we can see that the host galaxy extinction (EBVhost) is strongly negatively correlated with the distance modulus (DM) which does not come as a surprise. As the host extinction (the reddening of the light) increases then the apparent magnitude of the supernova that can be observed decreases. This may also be related to the fact that in the standard fitting, these two parameters are the only ones with systematic errors \cite{snoopy_online_docs} - the systematics in the distance modulus includes the intrinsic dispersion of the Phillips relation. 

With the other parameters, we can see from the covariance plots that they have very little correlation to each other. This can be extended for the relations between dm15-DM and EBVhost-Tmax which we have not presented, they too show no correlations.

The point to make here then is that we should have constrained the parameters for the host extinction or for the distance modulus. We gave SNooPY free reign in finding the parameters, using methods such as minimising $\chi^2$ and calling up the Schlegel maps to find the extinction with our given RA and dec. 

This does mean that we can now compare SNooPy's output with known information and use this as an extra judge of the quality of fit. Focussing on the results for 2017hhz, we begin with the distance modulus. For \texttt{fit()} and \texttt{fitMCMC()} we get $36.121\pm0.106$ mag and $36.096\pm0.113$ mag, and retrieved from NED the Tully-Fisher determined value for UGC 01218 is $35.64\pm0.43$ mag \cite{tully_fish_catalogue}. Comparing these values we see that whilst they are not completely different, within the SNooPy uncertainties, they do not agree with the Tully-Fisher distance modulus.
 
A similar comparison can be attempted for the host extinction. SNooPy produced $0.049\pm0.083$ and $0.047\pm0.067$, whilst a galactic extinction of 0.043 can be quoted from Schlafly and Finkbeiner (2011) \cite{reddening}. As the statistical uncertainties are larger than the actual values obtained, this further highlights the need that we should have initially constrained the templates.

We find that overall, whilst SNooPy is able to produce templates and fit them to a certain degree of accuracy, steps have to be taken to ensure that this accuracy can be further improved. Steps such as providing our own constraining parameters, and also using the MCMC based fitter as that provides a more accurate result than \texttt{fit()}.

% Not sure that MCMC is more accurate, write something about that!

\vspace{-2ex}
\subsubsection{Can we use our results for cosmology then?}
\vspace{-2ex}

We have the parameters from our final SNooPy fittings and we concluded that they are not entirely inaccurate, could we use these them for cosmology then?

Take calculating a value for Hubble's constant using 2017hhz, we have all the required information: the redshift from other astronomers and our distance modulus from SNooPy. If we took Equation \ref{eqn:h_0}, Equation \ref{eqn:redshift}, and the following equation, 
\begin{equation}
\mu = 5 \log_{10}(d) - 5,
\end{equation}

where $\mu$ is the distance modulus in magnitudes and $d$ is the distance in parsecs \cite{mod_ast}. Using that and a value for speed of light in a vacuum from NIST \cite{speed_of_light} we would be able to produce $H_0=70\pm20$ km s$^{-1}$ Mpc$^{-1}$.

Comparing this to existing measurements, $H_0 = 73.24 \pm 1.74$ km s$^{-1}$ Mpc$^{-1}$ \cite{hubble_constant}, we see that whilst it appears to be consistent, the uncertainty entirely negates this fact. In our calculations we have had to estimate the uncertainty on the redshift and use SNooPy's provided statistical uncertainty for the distance modulus.

If we wanted to use our results for cosmology we would firstly have to improve the original accuracy of our fitting and we would also have to find a value for the redshift which has a known uncertainty.  
 
However, we must be aware that calculating $H_0$ is a partly cyclic process when using SNooPy. For the production of the Type Ia templates, cosmological parameters ($H_0$, $\Omega_M$, and $\Omega_{\Lambda}$) were employed in the calibration of the absolute magnitudes of their sample of Type Ia supernovae \cite{car_snoopy} . Whilst these parameters are not used in the actual fitting, we note that they might introduce an uncorrectable bias in our results. 

In answering our question, we could use our results for cosmology, but we would have to introduce improvements and additional uncertainties. 

We now arrive at a point where we have discussed the limitations and have judged the quality of fit that SNooPy provides, now we turn to the instruments that we used in collecting our data, the telescopes.

\vspace{-2ex}
\subsection{Observing supernovae}
\vspace{-2ex}

In collecting data for this experiment we had to be aware of the sources of noise that would arise and do our best to account for them. We had accounted for the uncertainties for bias and dark current, flat-fielded our data to remove the variations in sensitivity of the CCD pixels, and stacked our data to improve the supernova signal. 

But from our SNooPy results we see that we required an improved data set for more accurate values. An important case to make here is if we were to repeat this experiment for new supernovae, we should use telescopes with a larger aperture. This is so that we could collect more data as the supernova dims, filling out that decline path and improving our SNooPy result. 

On the other hand, an attempt could be make to take even more frames in the dimming period of the supernova. When the time came to combine the data, the fainter signals would be improved through this method.

Using a longer exposure time could also be considered, but then there is a possibility that the images would become saturated and in a situation where our targets are already faint, this would not be ideal.

For this, it would be helpful to know what the magnitude limit for our telescopes are. A quantifiable value is difficult to produce as observations and the data rely on the conditions at the time of viewing, exposure time, exposure number, to put a certain value is difficult. Nevertheless we can use our data and final graphs from SNooPy to try and put a rough estimate on the smallest magnitude which can be viewed.

What we see from Table \ref{table:2017hhz_data}, Table \ref{table:2017hle_data} and Figure \ref{fig:2017hle_lc}, as the magnitude reaches around and below $18$ mag the uncertainties increase. While we definitely can attribute this to less photons being detected as the supernova dims, we can additionally suggest that this could be the case as the telescopes are reaching their observation limit. Thus we can give a value of $\sim 18$ mag for the limits of Far-East-16 and pt5m.

In highlighting the difference with the types of data that we collect, in Figure \ref{fig:2017hhz_collage} we present a collage of data that we used. At a first glance of Figure \ref{fig:2017hhz_collage}(c)  we note that flat-fielding our data vastly improves the quality of it. It cannot be seen directly but the vignette effect has been removed, cosmic rays taken out, and the data has reduced noise in general. 

With the comparison of the raw data, the first thing to note is that different observation conditions would have been present when the data was being taken. In Figure \ref{fig:2017hhz_collage}(a) five exposures each taken for 120s had been taken in La Palma. For Figure \ref{fig:2017hhz_collage}(b) the Durham site was used for 24 exposures at 60s each. On both days the conditions appeared to be clear. 

We do notice in Figure \ref{fig:2017hhz_collage}(a) that all the objects appear to be slightly warped like a rugby ball, this potentially could be due to atmospheric turbulence. It was not atypical that our data would sometimes have aberrations such as this. We can cite some cases where a satellite or an aeroplane had transited through some of our frames. It was not until we had stacked the multiple frames together that we would notice such a thing had occurred.

This is another advantage for collecting as many frames of our objects as possible. In addition to reducing unwanted noise, we could circumnavigate the problems that erroneous images can bring. 

Looking away from data collection, we can instead suggest improvements which could be made instead to our photometry methodology. We have previously discussed that we performed the photometry manually and this should have been made an automated process. We restate this and further suggest that for our calibration objects we should have used more than two so that the accuracy of our zero-points would be improved, and in turn, our apparent magnitudes.

Now, considering the tools we used for this photometry, GAIA was our main astronomical data analysis package. For the counts of our objects we did not make use of the quoted uncertainty from the program, instead we chose a functional approach which can be found in Appendix \ref{app:uncertainties}. A method opted for as we had control and an understanding of how it worked.

GAIA's other measurement parameter that we utilised was the estimator for the sky background value. A value which we only used for our signal-to-noise calculations when we chose the radius of our apertures. The way in which this value is calculated can be changed within GAIA's photometry settings. Our calculations in particular make use of the `clipped mean', a mean which disregards the extreme high and low values. Compared with the standard `mean', there is only a $\sim 0.06 \%$ difference between both values, a difference too small to be a problem in our calculations.

Once we had gained a strong set of skills in operating telescopes and collecting plus analysing data we attempted to apply what we had learnt to other related projects. 

\vspace{-2ex}
\subsection{Galaxy subtraction, the search for transients, and closing thoughts}
\vspace{-2ex}

Using the data which we had previously collected we attempted to perform a `galaxy subtraction' on our 2017hhz frames. Our intention was to remove the background galaxy, UGC 01218, and only leave the bright supernova behind. The method consisted of interpolating two frames, one where the supernova is really bright and one where it has faded from view, then one of the images had to be accounted for the point spread function (PSF) of the other. This involved convolving a Gaussian function with one of the frames so that the objects in both had the same `spread'. The images could then be subtracted from each other. In Figure \ref{fig:2017hhz_collage}(d) we see that our attempt was partially successful. We successfully removed the galaxy, but we did not manage to convolve our Gaussian function by the correct amount, thus leaving behind `negative' objects in the place of the stars. 

If we had been successful in implementing this galaxy subtraction, we then would have rudimentarily attempted our own transients survey. Throughout the project, in addition to our supernova observations, we viewed areas where we thought supernovae may occur. We initially started with stellar clusters, Messier 7 and Messier 10, but we soon realised that there would be a low chance for their stars to explode. So, we took our observations to a galactic cluster, the Perseus Cluster (Abell 426). 

We had intended to carry out a `background subtract' on our frames, new events would then be easily detected. We state these past intentions instead of presenting results as there was just not enough time. Our time was limited and in the end we decided it would be more important for us to focus our efforts on the main project, producing Type Ia light curves. 

Looking at our objects log and observing logs we find that we observed many more objects than just 2017hhz and 2017hle. Not everything that we collected could be analysed as it would take an excessive amount of time to do so. 

We therefore suggest that if this project were to be revisited in the future, it would help if the photometry and the template fittings were automated. In doing so, more time could be devoted to focussing on the astrophysics behind supernovae and their applications. We also suggest that other fitters should be used as well as SNooPy, namely SALT2 and MLCS2k2, it would certainly be interesting to compare and contrast the uncertainties in their results and their initial parameters.

\vspace{-3ex}
\section{Conclusions}
\label{conclusions}
\vspace{-2ex}

We asked ourselves at the beginning of this experiment whether it would be possible for us to observe Type Ia supernovae and reproduce their evolutionary light curves. Our conclusion is that it is possible, but the accuracy and uncertainties could certainly be improved. 

Through the usage of SNooPy we managed to apply models to our data which were not entirely unacceptable, producing average reduced $\chi^2$ values of $\sim 1.49$ for the B band and $\sim 2.88$ for the V band. The calculated parameters used in the fittings, the distance modulus and galactic extinction, are consistent with known values, however, their uncertainties negate any agreement as the results become meaningless. 

Therefore adjustments must be made to produce more reasonable uncertainties. With our observational method, an extensive data set should be collected with a larger telescope, one which covers the entire evolution of the supernova. Further, for our analysis, if SNooPy is to be used again for our fittings, known constraints should be placed on the templates as this would in turn produce more accurate results.

Having a better set of results in hand, one could then achieve more consistent values for the geometry of the universe. We shall state once again that Type Ia supernovae are an important tool for cosmology, and it is important to produce accurate light curves.

\vspace{-3ex}
\section*{Acknowledgements}
\vspace{-2ex}
We thank Dr. J. Lucey, Dr. M. Swinbank, and U. Dudzeviciute for the continual support and help which was provided over our entire experimental period. We also thank the staff at the Department of Physics at Durham University for the upkeep of the department's roof telescope array, and we thank Dr. R. Wilson for the usage of the robotic telescope at La Palma. 

This research has made use of the CfA Supernova Archive, which is funded in part by the National Science Foundation through grant AST 0907903.

The Starlink software (Currie et al 2014) is currently supported by the East Asian Observatory.

As a final show of gratitude, I thank Duncan Middlemiss for his patience and partnership during this project.

\bibliographystyle{abbrv}
\bibliography{supernovae}

\clearpage
\onecolumngrid
\vspace{-3ex}
\appendix
\section{Objects Log} \label{app:objects_log}
\vspace{-2ex}
A list of the objects that were chosen to be observed, and then the subsequent notes on them. Not all objects were chosen to be observed for an extended period, the ones noted were observed for a couple of nights to ensure suitability. The subsequent observation logs can be found in Appendix \ref{app:observations_log}.

{\renewcommand{\arraystretch}{1.2}%
\begin{table}[h!]
\centering    
\begin{tabularx}{\textwidth}{c c c c @{\hskip 5pt} c c X}
    \hline
    \textbf{Object} & \textbf{RA} & \textbf{Dec} & \textbf{Magnitude} &\textbf{First Discovered} &\textbf{Type} & \textbf{Notes} \\ 
    *ASASSN-17mz & 23:56:21.82 & +32:27:24.08 & 14.6 & 2017/09/30.500 & Ia & {Too close to galactic nucleus, cannot see}  \\
    *AT2017hld & 22:18:22.849 & +34:45:08.46 & 16.1 & 2017/10/17.339 & CV & {Cataclysmic Variable, stopped observing}  \\
    *2017hky & 11:23:30.514 & +63:21:59.43 & 16.2 & 2017/10/16.640 & II & {Not viewable from Durham or La Palma}  \\
    2017hhz & 01:44:16.75 & +12:15:18.00 & 16.83 & 2017/10/16.140 & Ia & {A measured redshift, $z=0.0392$}  \\
    AT2017gvb & 08:04:42.34 & +61:31:41.50 & 17.33 & 2017/09/26.59 & unk & {-}  \\
    *ASASSN-17nb & 07:27:37.32 & +35:36:28.30 & 17.31 & 2017/09/25.59 & II & {Object is dwarfed by brightness of the galaxy}  \\
    2017hle & 01:07:36.060 & +32:24:30.00 & 18.0 & 2017/10/18.684 & Ia-91bg & {-}  \\
    2017hou & 04:09:02.140 & -01:09:36.40 & 17.9 & 2017/10/24.370 &Ia & {Viewable from La Palma}  \\
    *AT2017hmw & 01:07:16.570 & +31:25:28.88 & 17.2 & 2017/10/19.415 & CV & {Cataclysmic Variable}  \\
    2017hpa & 04:39:50.750 & +07:03:54.90 & 17.9 & 2017/10/15.346 & Ia & {Viewable from La Palma}  \\
    *AT2017hnm & 01:42:03.24 & +42:31:08.50 & 16.69 & 2017/10/23.44 & unk & {Another star in the image dwarfs the SN in brightness}  \\
    AT2017hpm & 08:04:15.100 & -00:03:58.03 & 16.4 & 2017/10/26.290 & unk & {-}  \\
    *AT2017hqa & 01:08:59.160 & +32:38:04.10 & 17.3 & 2017/10/26.740 & unk & {Unobservable, too close to galactic centre}  \\
    2017hqc & 23:23:08.210 & +10:38:54.63 & 18.0 & 2017/10/27.490 & Ia & {-}  \\
    *AT2017hrr & 11:29:06.490 & -08:59:18.56 & 15.4 & 2017/10/30.607 & unk & {Cannot view from Durham or La Palma}  \\
    *AT2017hhq & 00:42:50.230 & +41:15:27.10 & 17.7 & 2017/10/30.599 & NV & {A nova close to M31}  \\
    AT2017htb & 22:09:38.520 & +17:39:39.56 & 15.7 & 2017/11/02.190 & unk & {-}  \\
    AT2017hzw & 02:05:18.337 & +53:12:13.16 & 17.3 & 2017/11/13.410 & Ia-CSM & {-}  \\
    2017igf & 11:42:49.850 & +77:22:12.94 & 15.4 & 2017/11/18.590 & Ia & {Promising Type Ia target}  \\
    \hline      
\end{tabularx}
\caption{Objects that we chose to observe and notes on them. RA is the Right Ascension, given in units of hours : arcminutes : arcseconds. Dec is the Declination, degrees : minutes : seconds. The stated magnitude is the initial magnitude that the object was discovered in the V band. Objects marked with an asterisk $*$ were objects which we chose to stop observing, the reason provided in the notes.}
\label{objects}
\end{table}


\clearpage

\onecolumngrid
\vspace{-3ex}
\section{Observation Logs} \label{app:observations_log}
\vspace{-2ex}
Given below are all the observations which were made during our observation periods. The exposures column is of the following format: ($x: y, z$ s), where $x$ is the band in which the images were taken in, $y$ the number of exposures taken, and $z$ the exposure time which was used, in units of seconds.
{\renewcommand{\arraystretch}{1.2}%
\begin{table}[h!]
\centering    
\begin{tabularx}{\textwidth}{c@{\hskip 5pt} c c@{\hskip 5pt} c@{\hskip 5pt} c@{\hskip 5pt} X}
    \hline
    \textbf{Date} & \textbf{Object} & \textbf{Time} & \textbf{Exposures} & \textbf{  Conditions  } & \textbf{Notes} \\ 
    20/10/17 & 2017hhz & 22:25:34 to 22:58:50 & \makecell{B: 5, 120s \\ V: 5, 120s} & {Clear} & {pt5m: -}  \\
    	& ASASSN-17nb &  02:56:08 to 03:23:36 & \makecell{B: 5, 60s \\ V: 5, 60s} & {Cloudy} & {pt5m: Data was unusable due to noise from cloud.} \\      
	
    21/10/17 & - & - & - & Cloudy & {\em No observations: weather not sufficient in Durham or La Palma for observations. \em} \\
    
    22/10/17 & AT2017hmw &  21:52:20 to 22:31:49 & \makecell{B: 5, 120s \\ V: 5, 120s} & {Clear} & {pt5m: -} \\  
    & 2017hhz & 22:40:20 to 23:19:50 & \makecell{B: 4, 60s \\ V: 12, 60s} & {Clear} & {pt5m: -}  \\
    & ASASSN-17nb &  02:42:45 to 03:10:46 & \makecell{B: 5, 120s \\ V: 5, 60s} & {Clear} & {pt5m: -} \\  
    & AT2017gvb & 03:12:31 to 03:58:58 & \makecell{B: 5, 180s \\ V: 5, 120s} & {Clear} & {pt5m: -} \\    
    
    23/10/17 & 2017hhz & 22:53:12 to 23:32:42 & \makecell{B: 5, 120s \\ V: 5, 120s} & {Cloudy} & {pt5m: Data produced has FWHM ranging from $1.6$ to $9.9$.} \\  
    
    24/10/17 & 2017hhz & 22:44:01 to 23:23:30 & \makecell{B: 5, 120s \\ V: 5, 120s} & {Cloudy} & {pt5m: Data is noisy, potentially another object transited across the frame while observing.} \\  
    
    25/10/17 & - & - & - & {Cloudy} & {\em No observations: too cloudy for observations in Durham, and items in pt5m queue were pushed out in favour of other objects. \em} \\
    
    26/10/17 & 2017hle & 20:54:44 to 21:09:03 & \makecell{B: 4, 60s \\ V: 4, 60s} & {Clear} & {FE16: - }  \\
    & 2017hhz &  21:18:00 to 21:25:31 & \makecell{B: 4, 60s \\ V: 4, 60s} & {Clear} & {FE16: -} \\ 
    & AT2017hmw &  21:29:15 to 21:39:36 & \makecell{B: 4, 60s \\ V: 4, 60s} & {Cloudy} & {FE16: -} \\
    & Messier-7 &  21:50:23 to 21:54:47 & {C: 1, 60s} & {Slightly Cloudy} & {FE16: Test object for SN discovery.} \\
    & Messier-10 &  21:55:47 to 22:00:13 & {C: 1, 60s} & {Cloudy} & {FE16: Test object for SN discovery.} \\
    & AT2017hnm &  23:27:22 to 23:41:55 & \makecell{B: 5, 120s \\ V: 3, 120s} & {Clear} & {pt5m: -} \\ 
    & AT2017gvb &  - & - & {-} & {pt5m: Object pushed out of the queue in favour of others.} \\

    27/10/17 & AT2017hqa & 18:48:12 to 18:52:59 & \makecell{B: 4, 60s} & {Cloudy} & {FE16: Object too close to galactic nucleus. }  \\
    & AT2017hnm &  18:56:34 to 19:00:23 & \makecell{C: 1, 60s} & {Slightly Cloudy} & {FE16: Object too dim, cloud cover reduced light we were receiving.} \\
    & Abell 426 &  19:04:23 to 19:15:41 & \makecell{C: 1, 60s \\ B: 9, 60s \\ V: 9, 60s} & {Clear} & {E14: Object to observe to discover new SN.} \\
    & 2017hhz &  20:11:35 to 20:22:43 & \makecell{B: 4, 60s \\ V: 4, 60s} & {Cloudy and Windy} & {E14: Seeing is bad due to the weather.} \\
    & 2017hle &  20:26:18 to 20:37:35 & \makecell{B: 4, 90s \\ V: 4, 90s} & {Slightly Cloudy} & {E14: Seeing is bad in these images as well.} \\
    & 2017hou &  23:48:09 to 23:50:14 & \makecell{B: 2, 120s} & {-} & {pt5m: Only two frames taken, not sufficient data.} \\
    \hline      
\end{tabularx}
\label{obs_logs1}
\end{table}

{\renewcommand{\arraystretch}{1.2}%
\begin{table}[h!]
\centering    
\begin{tabularx}{\textwidth}{c@{\hskip 5pt} c c@{\hskip 5pt} c@{\hskip 5pt} c@{\hskip 5pt} X}
    \hline
    \textbf{Date} & \textbf{Object} & \textbf{Time} & \textbf{Exposures} & \textbf{  Conditions  } & \textbf{Notes} \\ 
    27/10/17 & AT2017gvb &  02:16:01 to 03:03:05 & \makecell{B: 5, 120s \\ V: 5, 120s} & {Clear/Cloudy} & {pt5m: Some images are more noisy due to clouds.} \\
    
    28/10/17 & AT2017hnm &  21:41:55 to 22:21:25 & \makecell{B: 5, 120s \\ V: 5, 120s} & {Clear} & {pt5m: -} \\
    & 2017hhz &  21:28:50 to 21:30:50 & \makecell{B: 1, 120s} & {Clear} & {pt5m: -} \\
    
    29/10/17 & 2017hle &  21:22:39 to 21:41:21 & \makecell{B: 10, 120s \\ V: 1, 120s} & {Clear} & {pt5m: Mistake on our part to take so many images in B band, and only one in V band.} \\
    & AT2017hnm &  21:55:34 to 22:35:04 & \makecell{B: 5, 120s \\ V: 5, 120s} & {Slightly Cloudy} & {pt5m: -} \\
    & 2017hhz &  23:16:48 to 23:56:18 & \makecell{B: 5, 120s \\ V: 5, 120s} & {Clear} & {pt5m: -} \\
    & 2017hou &  00:33:33 to 01:13:02 & \makecell{B: 5, 120s \\ V: 5, 120s} & {Clear} & {pt5m: -} \\
    & 2017hqc &  01:16:20 to 01:41:20 & \makecell{B: 4, 120s \\ V: 4, 120s} & {Clear} & {pt5m: -} \\
    & AT2017gvb &  02:18:51 to 02:55:47 & \makecell{V: 13, 180s} & {Clear} & {pt5m: Long exposure and large number of exposures chosen as a test to see the quality of stacking them.} \\
    
    30/10/17 & 2017hle &  21:18:56 to 21:58:25 & \makecell{B: 10, 120s \\ V: 10, 120s} & {Clear} & {pt5m: -} \\
    & 2017hhz &  22:06:34 to 22:46:04 & \makecell{B: 5, 120s \\ V: 5, 120s} & {Clear} & {pt5m: -} \\
    & AT2017gvb &  02:04:22 to 03:02:50 & \makecell{V: 20, 180s} & {Clear} & {pt5m: An human error, we re-selected this object from the previous night instead of the one with the correct number of required frames for each band.} \\
    
    31/10/17 & - & - & - & {Cloudy} & {\em No observations: too cloudy for observations in Durham and La Palma. \em} \\
    
    01/11/17 & 2017hle & 23:56:53 to 00:15:35 & \makecell{B: 5, 120s \\ V: 5, 120s} & Clear & {pt5m: -} \\
    & AT2017hnm & 00:18:23 to 00:57:55 & \makecell{B: 5, 120s \\ V: 5, 120s} & Clear & {pt5m: -} \\
    & 2017hhz & 01:00:47 to 01:40:17 & \makecell{B: 5, 120s \\ V: 5, 120s} & Clear & {pt5m: -} \\
    & 2017hou & 01:43:38 to 02:23:08 & \makecell{B: 5, 120s \\ V: 5, 120s} & Clear & {pt5m: -} \\
    
    02/11/17 & - & - & - & {Cloudy} & {\em No observations: too cloudy for observations in Durham and La Palma. \em} \\
    
    03/11/17 & - & - & - & {Cloudy} & {\em No observations: too cloudy for observations in Durham and La Palma. \em} \\
    
    04/11/17 & - & - & - & {Cloudy} & {\em No observations: too cloudy for observations in Durham and La Palma. \em} \\
    
   05/11/17 & ASASSN-17mz & 17:53:45 to 18:09:34 & \makecell{B: 8, 60s \\ V: 8, 60s} & {Clear} & {FE16: -} \\
   & 2017hhz & 18:50:39 to 19:24:43 & \makecell{B: 8, 120s \\ V: 8, 60s} & {Clear} & {FE16: -} \\
   & AT2017htb & 19:38:14 to 20:02:16 & \makecell{B: 8, 60s \\ V: 8, 60s} & {Slightly Cloudy} & {W14: -} \\
   & 2017hle & 20:05:49 to 20:08:28 & \makecell{V: 2, 60s} & {Cloudy} & {W14: Stopped taking exposures after the weather worsened.} \\
   & 2017hqc & 20:14:16 to 20:18:25 & \makecell{V: 3, 60s} & {Cloudy} & {W14: Cleared up for a brief moment and then re-clouded over.} \\
   & AT2017hhq & 20:33:58 to 20:42:26 & \makecell{B: 4, 60s \\ V: 4, 60s} & {Clear} & {W14: -} \\
       \hline      
\end{tabularx}
\label{obs_logs2}
\end{table}

{\renewcommand{\arraystretch}{1.2}%
\begin{table}[h!]
\centering    
\begin{tabularx}{\textwidth}{c@{\hskip 5pt} c c@{\hskip 5pt} c@{\hskip 5pt} c@{\hskip 5pt} X}
    \hline
    \textbf{Date} & \textbf{Object} & \textbf{Time} & \textbf{Exposures} & \textbf{  Conditions  } & \textbf{Notes} \\ 
    05/11/17 & AT2017hhq & 19:38:14 to 20:02:16 & \makecell{B: 3, 30s \\ V: 3, 30s \\ R: 3, 30s} & {Cloudy} & {E14: Images taken so that we can produce a colour image. } \\
    
    06/11/17 & - & - & - & {Cloudy, and Rain} & {\em No observations: too cloudy for observations in Durham, and torrential rain in La Palma. \em} \\
    
    07/11/17 & - & - & - & {Cloudy and Rain} & {\em No observations: too cloudy for observations in Durham, and torrential rain in La Palma. \em} \\
    
    08/11/17 & - & - & - & {Cloudy and Rain} & {\em No observations: too cloudy for observations in Durham, and torrential rain in La Palma. \em} \\
    
    09/11/17 & - & - & - & {Cloudy} & {\em No observations: too cloudy for observations in Durham, La Palma was not functioning. \em} \\
    
    10/11/17 & 2017hhz & 19:15:32 to 19:31:57 & \makecell{B: 4, 90s \\ V: 2, 90s} & {Cloudy} & {FE16: -} \\
    & 2017hqc & 19:37:27 to 19:49:15 & \makecell{B: 4, 90s \\ V: 2, 90s} & {Cloudy} & {FE16: -} \\
    
    11/11/17 & AT2017htb & 19:08:41 to 19:26:15 & \makecell{B: 5, 60s \\ V: 8, 60s} & {Clear} & {FE16: -} \\
    & 2017hhz & 19:34:43 to 19:43:48 & \makecell{B: 5, 60s \\ V: 4, 60s} & {Clear} & {FE16: -} \\
    & 2017hle & 19:45:56 to 19:53:46 & \makecell{B: 4, 60s \\ V: 4, 60s} & {Clear} & {FE16: -} \\
    & 2017hqc & 19:55:31 to 20:02:59 & \makecell{B: 4, 60s \\ V: 4, 60s} & {Clear} & {FE16: -} \\
    & Abell 426 & 20:04:54 to 20:08:03 & \makecell{B: 4, 60s} & {Clear} & {FE16: -} \\
    & 2017hhz & 20:09:53 to 20:34:55 & \makecell{B: 15, 60s \\ V: 7, 60s} & {Clear} & {FE16: It was made clear that having more frames would improve the quality of the final stacked frame. So more frames for this supernova were made.} \\
    
    12/11/17 & 2017hhz & 20:35:15 to 20:53:08 & \makecell{V: 17, 60s} & {Clear} & {FE16: As the night progressed, conditions became more cloudy so we were unable to take in the B band.} \\
    & 2017hle & 21:03:54 to 21:22:36 & \makecell{B: 5: 120s \\ V: 5, 120s} & {Clear} & {pt5m: We decided to input our remaining targets into the telescope in La Palma for observation.} \\
    & 2017hqc & 21:26:00 to 21:57:13 & \makecell{B: 4: 120s \\ V: 4, 120s} & {Clear} & {pt5m: -} \\
    
    15/11/17 & 2017hhz & 21:01:09 to 21:40:40 & \makecell{B: 5, 120s \\ V: 5, 120s} & {Clear} & {pt5m: -} \\
    & 2017hou & 00:16:29 to 00:56:01 & \makecell{B: 4: 120s \\ V: 4, 120s} & {Clear} & {pt5m: -} \\
    & AT2017gvb & 01:33:35 to 02:32:04 & \makecell{V: 20, 180s} & {Clear} & {pt5m: We made a mistake with our entry into the telescope queue- we selected too high of an exposure number.} \\
    
    16/11/17 & 2017hhz & 19:19:46 to 20:11:30 & \makecell{B: 8, 180s \\ V: 8, 180s} & {Clear} & {FE16: -} \\
    & 2017hle & 20:20:05 to 20:48:31 & \makecell{B: 8, 90s \\ V: 8, 90s} & {Clear} & {FE16: -} \\
    & Abell 426 & 21:01:35 to 21:17:03 & \makecell{B: 18, 30s} & {Clear} & {FE16: -} \\
    
    17/11/17 & 2017hhz & 18:52:28 to 19:49:50 & \makecell{B: 8, 90s \\ V: 8, 90s} & {Clear} & {FE16: -} \\
    & 2017hle & 19:52:09 to 20:15:48 & \makecell{B: 8, 90s \\ V: 8, 90s} & {Clear} & {FE16: -} \\
    & AT2017htb & 20:18:47 to 20:29:42 & \makecell{B: 4, 90s \\ V: 4, 90s} & {Clear} & {FE16: -} \\
    
       \hline      
\end{tabularx}
\label{obs_logs3}
\end{table}

{\renewcommand{\arraystretch}{1.2}%
\begin{table}[h!]
\centering    
\begin{tabularx}{\textwidth}{c@{\hskip 5pt} c c@{\hskip 5pt} c@{\hskip 5pt} c@{\hskip 5pt} X}
    \hline
    \textbf{Date} & \textbf{Object} & \textbf{Time} & \textbf{Exposures} & \textbf{  Conditions  } & \textbf{Notes} \\ 
    17/11/17& AT2017hzw & 20:40:43 to 21:00:19 & \makecell{B: 16, 60s} & {Cloudy} & {FE16: Stopped observations as it had become cloudy.} \\
    
    18/11/17 & 2017hle & 18:13:52 to 19:01:03 & \makecell{B: 15, 60s \\ V: 24, 60s} & {Clear} & {FE16: -} \\
    & 2017hhz & 19:03:47 to 19:45:56 & \makecell{B: 16, 60s \\ V: 24, 60s} & {Clear} & {FE16: -} \\
    
    23/11/17 & 2017hhz & 19:38:31 to 20:34:04 & \makecell{B: 16, 60s \\ V: 24, 60s} & {Clear} & {FE16: -} \\
    & 2017hle & 20:36:52 to 21:00:00 & \makecell{V: 15, 90s} & {Clear} & {FE16: Clouded over at the end of our observations for V band.} \\
       \hline      
\end{tabularx}
\caption{Observation logs for the entire observation period for our experiment.}
\label{obs_logs4}
\end{table}


\clearpage

\twocolumngrid
\vspace{-3ex}
\section{Using SNooPy} \label{app:using_snoopy}
\vspace{-2ex}
\textit{A brief overview and tutorial of how SNooPy can be used to fit Type Ia light curves.} \\

\textit{Example files can be obtained from either the Carnegie Supernova Project} \url{https://csp.obs.carnegiescience.edu/data/snpy} \textit{or from} \url{https://community.dur.ac.uk/jacky.cao/#l3lp}. \\

For SNooPy to fit Type Ia light curves, the correct file format to store our data was required. Below we give an example of the type of file that would contain B and V band data, \texttt{supernova.txt}.
\begin{center}
supernova.txt
\line(1,0){245}
\squeezeup
\squeezeup
\end{center}
\begin{lstlisting}
sn_name sn_redshift sn_ra sn_dec
filter Bs
date_1   bmag_1   bmag_err_1
date_2   bmag_2   bmag_err_2
date_3   bmag_3   bmag_err_3
filter Vs
date_1   vmag_1   vmag_err_1
date_2   vmag_2   vmag_err_2
date_3   vmag_3   vmag_err_3
\end{lstlisting}
\begin{center}
\squeezeup
\squeezeup
\squeezeup
\squeezeup
\line(1,0){245}
\end{center}

We now detail the steps that we took when using this Python package.

\subsection{A basic fit with \texttt{fit()}}
\begin{enumerate}
 \item SNooPy was initially loaded into a command line interface in the folder of \texttt{supernova.txt} with the command \texttt{snpy}.
 \item We then loaded the supernova text file with,
 \begin{equation*}
 \texttt{s = get\_sn(`supernova.txt')} .
 \end{equation*}
 
 \item An optional step, but one which we chose to make was to use the Carnegie Supernova Project's passbands as they would include spectral corrections, \textit{S-corrections}, doing this for the B and V bands by using,
 \begin{equation*}
 \begin{split}
 \texttt{s = restbands[`Bs'] = `B' } , \\ 
 \texttt{s = restbands[`Vs'] = `V' } .
 \end{split}
 \end{equation*}
 
 \item Once this was done, the basic fit was performed, selecting the two photometric bands which we used,
 \begin{equation*}
 \texttt{s = s.fit([`Bs',`Vs'])} . 
 \end{equation*}
 
 A matplotlib interface would then appear and you could view the data which was plotted with their uncertainties plus the fitted light curve. The bands which were selected would correlate to what would be shown in the plots.
 
 \item We could then view the parameters that SNooPy chose when fitting the light curves,
 \begin{equation*}
 \texttt{s.summary()} .
 \end{equation*}
 
 The host galaxy extinction, time for B-maximum, the distance modulus, and the decline-rate parameter would be shown with their statistical uncertainties.
 
 \item To retrieve the fitted data we used,
 \begin{equation*}
 \texttt{s.dump\_lc()} .
 \end{equation*}
 
 This function would dump your supernova data and the light curve data in the same folder you were operating in. The file formats used are '\texttt{.dat}'.
\end{enumerate}

\subsection{A more complex fit with \texttt{fitMCMC()}}
\begin{enumerate}
 \item Instead of using the \texttt{fit()} function which uses the Levenberg-Marquardt method to fit the light curves, a Markov Chain Monte Carlo method can be used instead. To invoke a fit with this method, the following command would be used, 
 \begin{equation*}
 \texttt{s.fitMCMC()} .
 \end{equation*}
 
 It is worth noting that as this is a more complex method, it is sampling multiple probabilities, it will take more time to render a final plot. 
 
 \item To produce the covariance plots, the function \texttt{fitMCMC()} could be used. However, the \texttt{fit()} function must be called before doing so,
 \begin{equation*}
 \texttt{s = s.fit([`Bs',`Vs'])} . 
 \end{equation*}
 
 Once this function has been called the following command should then allow us to produce the covariance (triangle) plots,
 \begin{equation*}
 \texttt{s.fitMCMC(plot\_triangle = True)} . 
 \end{equation*}
 
 Again, this will take some time to render. 
 
 We also note that it may be case that the \texttt{corner} Python module needs installing before the covariance plots can be produced, in which case this must be done. 
 
 Furthermore, once this has been completed, one of the files in the SNooPy installation needs to be change to account for this new module, \texttt{sn.py}.
 
 \item To obtain actual values for the correlations, the full covariance matrix can be access with, 
 \begin{equation*}
 \texttt{s.model.C}.
 \end{equation*}
\end{enumerate}

N.B.: The full stops and commas at the end of the functions should not be included, they have been added for grammatical and aesthetic reasons.

For other types of Type Ia supernovae, a different form of the standard template has to be selected, that can be done with this command,
\begin{equation*}
\texttt{s.k\_version = `type'},
\end{equation*}

where \texttt{type} could be \texttt{91bg} to represent Type Ia-91bg supernovae.

\clearpage

\onecolumngrid
\vspace{-3ex}
\section{Set of sample data}
\vspace{-2ex}
sdfsdfsdfsd

\vspace{-3ex}
\section{Programs created}
\vspace{-2ex}
Accessible from [[link]], in Table [[ref]] we provide a detailed list of the programs that were created for our experiment.

\clearpage

\twocolumngrid
\vspace{-3ex}
\section{Uncertainties} \label{app:uncertainties}
\vspace{-2ex}
Our magnitude values had an associated uncertainty which was found by using the following equation,
\begin{equation}
\delta{m} = 2.5 \log_{10} \Big(1 + \frac{1}{\sqrt{C}} \Big),
\label{m_uncert}
\end{equation}

where $\delta{m}$ is the uncertainty on the magnitude, and $C$ is the value obtained for the counts of the supernova explosion.

What other sources of uncertainty was there?

SNooPy uncertainties on the parameters it uses

\clearpage

\end{document}