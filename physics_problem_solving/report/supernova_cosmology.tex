\documentclass[twocolumn]{revtex4}
\usepackage{graphics,graphicx,epsfig,amsmath,multirow,gensymb,commath,textcomp,natbib,blindtext,tabularx,array,makecell,threeparttable,amssymb,relsize,csquotes}
\usepackage{listings}
\usepackage[a4paper, left=1.85cm, right=1.85cm, top=1.85cm, bottom=1.85cm]{geometry}   
\usepackage[normalem]{ulem}
\newcommand{\squeezeup}{\vspace{-2.5mm}}

\usepackage{lipsum}

\def\bibsection{\section*{\refname}} 
\renewcommand{\thesubsection}{\alph{subsection}}

\renewcommand\theadalign{bc}
\renewcommand\theadfont{\bfseries}
\renewcommand\theadgape{\Gape[4pt]}
\renewcommand\cellgape{\Gape[4pt]}

\begin{document}

\textheight=26.385cm
%Change textheight as the last resort...

\title{Constraining the geometry of the Universe using Type Ia supernovae and statistical methods}
 
\author{Jacky Cao, Group C3, Physics Problem Solving \\ Date of report: 28/02/2018}

\begin{abstract}     
The geometry and content of the universe can be quantified in terms of cosmological parameters. We therefore wanted to test the usage of Type Ia supernovae and statistical methods to constrain these parameters. Our work has made use of the Supernova Cosmology Project's Union2.1 compilation of 753 supernovae to constrain several cosmological parameters. Our initial experimentation used a least squares method which yielded a dark energy density parameter of $\Omega_\Lambda=0.74\pm0.01$, this was deemed an acceptable fit with $\chi^2_\nu=2.4547$ - a good fit requires $\chi^2_\nu=1$. We then decided to explore the multi-parameter $\Lambda$ Cold Dark Matter model using a variant of the Markov Chain Monte Carlo method. With this we encountered problems such as local minima which were dealt with by limiting our search parameter space. Using the Quintessence Cold Dark Model we obtained the results: $\Omega_\Lambda=0.73\pm0.01$, $\Omega_k=-0.0029\pm0.009$, $\Omega_m = 0.20\pm0.02$, $\Omega_r = (4.4\pm0.7)\times10^{-6}$, and $w=-0.912\pm0.01$. From this description we then formed a model, comparing it with observational data we found $\chi^2_\nu=2.4166$ which is acceptable and can be deemed as an improvement on the least squares method. To refine our work, local minima should be the main focus of future experimentation, one can potentially implement `simulated annealing' to circumnavigate them. Plus future work could involve combining Type Ia data with Cosmic Microwave Background and Baryon Acoustic Oscillation data to further constrain the cosmological parameters and provide more accurate results.
\end{abstract}

\maketitle

\vspace{-3ex}
\section{Introduction and Theory} 
\vspace{-2ex}
In cosmology, one can argue that one of the most important observable events are supernova explosions. As a massive main sequence star runs out of nuclear fuel to burn, the equilibrium configurations which initially provided structure will cease to exist. What follows is a cataclysmic supernova explosion \cite{longair}. 

We can generally class supernova explosions into two separate groups, Type I and Type II supernovae. The main distinction arises due to the fact that Type I's have an optical spectra which contains no Balmer hydrogen features, whilst Type II supernovae do contain this hydrogen feature \cite{mod_ast}. 

Within these two subclasses there are further divisions which can be characterised through their spectra and through features as found in their light curves \cite{obs_phys_class_sn}. Light curves are a way to show the evolution of a supernova's magnitude as time passes. For example, with two of the subclasses of Type II supernovae, Type II-L and Type II-P, there is a rapid linear decrease in magnitude for the former, and with Type II-P we see a constant plateau magnitude after maximum \cite{mod_ast}.

For the application of supernovae in cosmology, we must turn to the subclass of Type Ia and understand how they form plus how they are useful. 

\vspace{-3ex}
\subsection{Type Ia Supernovae}
\vspace{-2ex}
It is accepted that the light curves of Type Ia supernovae are generally homogeneous \cite{posn}, this means that they can be utilised as standard candles which allows us another measure of cosmic distance. 

Their homogeneity arises due to the mechanism behind their explosion. The progenitor system for Type Ia's consist of a binary system with a primary white dwarf and secondary star which has a mass close towards the Chandrasekhar limit of $1.4 M_{\odot}$ \cite{mod_ast, posn}. The white dwarf will accrete matter from it's companion until it itself reaches the critical mass limit. After this point we would then expect the primary star to collapse into a neutron star, however this is not the case. Instead, we find that a supernova explosion occurs. An event which arises due to a massive disruption to the star's internal structure by a large amount of energy.

It is currently understood that as the white dwarf accretes matter it heats up and produces thermonuclear energy. This energy is achieved in the stellar interior at a temperature of $10^9$ K, as a result a disruption to the electron degeneracy pressure takes place. The star becomes gravitationally unstable from the nuclear energy and a total collapse into a neutron star is prevented \cite{longair, posn}.

This particular mechanism can be used to explain the standard profile of Type Ia light curves. Using these plots, the maximum B band photometric magnitude can be obtained with the following equation,
\begin{equation}
M_{\max}(B)=-21.726+2.698\Delta m_{15}(B),
\end{equation}

where $\Delta m_{15}(B)$ is the decline rate parameter or Phillips' parameter \cite{high_en_astro}. This is defined as the mean rate of decline of the B band light curve from peak light to 15 days after the maximum \cite{abs_phil}. This parameter relates the apparent magnitude to time and provides a way to compare and contrast different Type Ia supernovae. 

With values for the absolute magnitude of Type Ia supernovae and with values of their redshift obtained from spectroscopic observations, the distance to these objects can then be calculated using the following equation, 
\begin{equation}
\mu = 5 \log_{10}(d) - 5,
\end{equation}

where $\mu$ is the distance modulus and can be defined as the absolute magnitude of supernova subtracted from it's apparent magnitude, and we define $d$ is the distance to the supernova in parsecs \cite{mod_ast}. 

From data sets of Type Ia supernovae we can easily employ them to calculate the geometry of the universe. But before that, we must first understand what cosmological parameters are and how they can be constrained.

\vspace{-3ex}
\subsection{Cosmological Parameters}
\label{sec:cosmo_parameters}
\vspace{-2ex}
At large cosmological distances, the appearance of objects will be affected by the spacetime which it travels through. If we therefore wanted to describe the geometrical properties of the universe, we would require to solve Einstein's general theory of relativity. From \textit{An Introduction to Modern Astrophysics} \cite{mod_ast} we can build the framework required to calculate and compute cosmological parameters. 

If we solve Einstein's field equations for an isotropic, homogenous universe then we can obtain a description of the evolution of the universe. Given as a differential equation it is more commonly known as the Friedmann equation. In the 1922 form of the equation, a non-static universe is accounted for,
\begin{equation}
\Big[ \Big( \frac{1}{R} \frac{dR}{dt} \Big)^2 - \frac{8}{3} \pi G \rho \Big] R^2 = -k c^2,
\label{eqn:1922_friedmann}
\end{equation}

where $R(t)$ is the scale factor, $G$ the gravitational constant, $\rho$ is the mass density, $k$ is a parameter which describes the shape of the universe, and $c$ is the speed of light in a vacuum.

$k$ can be defined as a constant which is equal to $-1$ for a negatively curved universe, $0$ for a spatially flat universe, and $+1$ for a positively curved universe \cite{longair}. 

Separate work performed by Einstein eventually led to a cosmological constant $\Lambda$ being introduced in the Friedmann equation. This was included in the form of $+\tfrac{1}{3}\Lambda c^2$ within the square brackets of the left hand side of Equation \ref{eqn:1922_friedmann}. Whilst Einstein included this term to account for his static and closed universe, as a consequence of observations which implied an accelerating universe, astronomers in the 1990s eventually related the cosmological constant to dark energy.

Taking the Friedmann equation, it can be rewritten so that it considers a three component universe composed of baryonic matter and dark matter, relativistic photons and neutrinos, and dark energy, 
\begin{equation}
\Big[ \Big( \frac{1}{R} \frac{dR}{dt} \Big)^2 - \frac{8}{3} \pi G (\rho_{M} + \rho_{r} + \rho_{\Lambda} )\Big] R^2 = -kc^2,
\end{equation}

where $\rho_{M}$ is the density of matter, $\rho_{r}$ is the density of radiation, and $\rho_{\Lambda}$ is the density of dark energy. Combining these densities with a critical density we can adapt the Friedmann equation so that it includes \textit{cosmological parameters}, these are values which describe the content makeup of the universe in terms of matter and energy. This critical density is defined as the value of the density that will result in a flat ($k=0$) universe. We see that the Friedmann equation becomes,
\begin{equation}
H^2 [1-(\Omega_m + \Omega_{r} + \Omega_{\Lambda})]R^2 = -kc^2,
\label{eqn:friedmann_old}
\end{equation}

where $H$ is the Hubble parameter and is related to the scale factor and expansion factors of the universe, $\Omega_m$ is the matter density parameter, $\Omega_{r}$ is density contribution from radiation, and $\Omega_{\Lambda}$ is defined as the dark energy density parameter.

With this in hand, it is now possible to calculate the proportions of the universe which amounts to matter and the amount which is energy. In these calculations, cosmologists normally choose to omit the contribution from radiation as it is a negligible amount. Results from the Wilkinson Microwave Anisotropy Probe (WMAP) show that $\Omega_{\text{rad}}=8.25 \times 10^{-5}$, an insignificant value when compared to $\Omega_{\Lambda}=0.74 \pm 0.04$ and $\Omega_m = 0.27 \pm 0.04$ \cite{mod_ast}. 

Using these cosmological variables we can define the total density parameter,
\begin{equation}
\Omega \equiv \Omega_m + \Omega_{\Lambda} + (\Omega_{r}).
\label{eqn:total_density}
\end{equation}

Inputting the WMAP data into Equation \ref{eqn:total_density} we see that a value of $\Omega \sim 1$ is produced, indicating that the universe is flat with $k=0$ and that dark energy is the dominant factor in the expansion of the universe \cite{mod_ast}.

This result was obtained through measurements of the cosmic microwave background, however it would be especially useful if data from other cosmic objects could be used. We therefore return to Type Ia supernovae as they are the perfect candidate to help us constrain the geometry of the universe. They are more readily accessible and observations can be more easily made. 

\vspace{-3ex}
\subsection{Cosmological Parameters from Type Ia Supernovae}
\vspace{-2ex}
To constrain the cosmological parameters using supernovae we do not look at the Friedmann equation directly. Instead, the main methodology is to use a Least Squares test to select the best model which would then correspond to cosmological parameters. We can define this test as the $\chi^2$ statistic, 
\begin{equation}
\chi^2 = \sum \frac{(f_\text{obs}-f_\text{model})^2}{\sigma_\text{obs}^2},
\label{eqn:chi_eqn}
\end{equation}

where $f_\text{obs}$ is the peak observed flux of the supernova, $f_\text{model}$ is the model flux for the supernova calculated using the corresponding redshift and with defined cosmological parameters, and $\sigma_\text{model}$ is the uncertainty on the observational flux \cite{script}.

Obtaining $f_\text{obs}$ is a simple matter, we can find it by using a rearranged form of,
\begin{equation}
m_\text{obs}=m_0 - 2.5\log_{10}(f_\text{obs}),
\label{eqn:mag_flux}
\end{equation}

that is, $m_\text{obs}$ is the effective peak magnitude of the supernova, $m_0$ is the instrumental zero point constant with a value of $-20.45$, and $f$ is the supernova flux in units of erg cm$^{-2}$s$^{-1}$\AA$^{-1}$ \cite{script}.

On the other hand, to find $f_\text{model}$ we begin by defining the flux equation for a certain model,
\begin{equation}
f_\text{model}=\frac{L_\text{peak}}{4\pi [R_0 S(\eta)]^2 (1+z)^2},
\label{eqn:fmodel}
\end{equation}

where $L_\text{peak}$ is the peak luminosity value for Type Ia supernovae, $R_0 S(\eta)$ is the comoving distance between the observer and where the supernova exploded in space, and $z$ is the redshift for the supernova \cite{script}.

To find $L_\text{peak}$, the low-redshift ($z<0.1$) Type Ia supernovae objects can be selected out of a data set and then Equation \ref{eqn:fmodel} can be used by taking the approximation $R_0 S(\eta) \approx cz/H_0$. Optimum $L_\text{peak}$ can then found by using Equation \ref{eqn:chi_eqn}, the smallest value for $\chi^2$ corresponds to the best luminosity. This value can then be applied to calculations involving supernovae which have redshifts higher than 0.1. For supernovae with $z>0.1$ the comoving distance cannot be simply approximated so we must use the following definition,
\begin{equation}
R_0 \eta = c \int_0^z \frac{dz'}{H(z')},
\label{eqn:comoving_integral}
\end{equation}

we find that within this integral we need to integrate a form of the Friedmann equation between a value for the redshift of the supernova $z$ and no redshift. We can derive this by using Equation \ref{eqn:friedmann_old} and with the assumption that if the universe is flat then $\Omega_m = 1 - \Omega_{\Lambda}$, this then reduces the cosmological parameters that we have to find \cite{script}. 

We now arrive at the following result for the Hubble Parameter,
\begin{equation}
H^2 = H_0^2 [(1+z)^3 - \Omega_{\Lambda} ((1+z)^3-1)],
\label{eqn:hubble_parameter}
\end{equation}

where $H_0$ is Hubble's constant, $z$ is the redshift for a supernova, and $\Omega_\Lambda$ is a cosmological constant.

To find the optimum value for $\Omega_\Lambda$ we can select a range from 0.0 to 1.0 and then calculate $f_\text{model}$ and $\chi^2$ for each number. Identical to finding $L_\text{peak}$, the best value for the cosmological parameter is found by choosing the minimum value for $\chi^2$. Through the usage of this least squares method one is able to explore cosmological parameters using Type Ia supernova data sets.

\vspace{-5ex}
\subsection{Project Aims}
\vspace{-2ex}
In this paper we discuss the experimental work performed to understand how cosmological parameters can be constrained using Type Ia supernova data. We explore Least Squares and Markov Chain Monte Carlo methods and then we discuss our exploration of more complex cosmological models which make use of several parameters. We finally conclude on potential work that could be done to improve the accuracy of our search.

\vspace{-3ex}
\section{Results and Discussion} 
\label{sec:results_discussion}
\vspace{-3ex}
\subsection{Least Square Statistics} 
\vspace{-2ex}
To calculate cosmological parameters using supernovae we must first obtain data sets of Type Ia supernovae which contain redshift, effective magnitude and the uncertainty on that magnitude. We specify effective magnitude as this means that it has been corrected for light curve stretching, galactic dust extinction and other factors \cite{script}. This means that the magnitudes used in our calculations will help us produce more accurate cosmological parameters. 

In our first experiments we chose to use a data of set of 42 high-redshift ($z>0.1$) Type Ia supernovae from the Supernova Cosmology Project (SCP) and 18 low-redshift ($z<0.1$) ones from the Cal\'{a}n Tololo Survey (CTS) \cite{dataset_1}. As indicated in our previous discussion of the experimental theory we require the low-redshift supernovae to `calibrate' the Type Ia data so that Equation \ref{eqn:fmodel} can be used. From Table \ref{table:cosmo_parameters} we found $L_{\text{peak}} = (3.1\pm0.1) \times 10^{35}$ W for the peak luminosity using the SCP and CTS data.

With this $L_\text{peak}$ we found $\Omega_\Lambda=0.86\pm0.04$. Comparing it to a literature value of $0.761^{+0.017}_{-0.018}$ \cite{cosmo_constraints} we find that there is only a $\sim 10 \%$ difference between them. One can gather that whilst our result is consistent with known values, within it's own uncertainties it does not include the observed value. This potentially could be the case as we calculated our $\chi^2$ values in what is known as \textit{flux}-space. Equation \ref{eqn:chi_eqn} uses the observational and model fluxes of the supernovae to calculate the $\chi^2$ values. Instead of this, magnitudes could be used which would shift us into \textit{magnitude}-space. In our investigations we attempted to recalculate $\Omega_\Lambda$ and found $0.71\pm0.04$, we see that this is more consistent to the given literature value.

Whilst we could have performed the rest of our experiment in magnitude-space, we chose not to as there would be additional implications and uncertainties.

In converting from fluxes into magnitudes using Equation \ref{eqn:mag_flux} we find that the transformation modifies the input flux distribution and the shape of the distribution will be biased towards fainter magnitudes \cite{magspace_stats}. However this does not account for the disparity between our calculated cosmological constants. 

Investigating this further, we have not been able to find the reason for this disparity in literature. Although we could suggest that the disparity may arise due to the conversion of flux into magnitude for the model to be in magnitude-space. If we consider observational data for supernovae, we find that multiple steps are required to convert an image into a magnitude value. First of all, frames are taken by CCDs and then aperture or point-spread function photometry is used to measure the amount of flux being received from the object. This flux then has to be converted into a magnitude using Equation \ref{eqn:mag_flux}, where the zero point is found by using objects from the same frame. 

In our work we have kept $m_0=-20.45$, a value which is constant no matter which supernova we are working with. If we were to change this to a more suitable value, would more accurate results be produced? Whilst this possibly could be the case, in our later research in flux-space and with using larger data sets we found that we could calculate cosmological parameters which were more consistent with measured values. This discrepancy between flux and magnitude-space could simply be a case of more data being required to constrain the variables. 

So, when we turned our research to using a larger data set in flux-space, from Table \ref{table:cosmo_parameters} we find that our value for $\Omega_\Lambda=0.74\pm0.01$. We see that there is a $\sim 3\%$ difference between it and the literature value suggesting that using a larger collection of Type Ia supernovae does help to produce a more accurate result. The data set used here is the Union2.1 Compilation from the SCP \cite{dataset_2}, it contains 753 Type Ia supernovae which should be able to provide us with a more accurate calculation when we use the Least Squares method. 

{\renewcommand{\arraystretch}{1.2}%
\begin{table*}[t]
\centering
\begin{tabular}{c@{\hskip 15pt}c@{\hskip 15pt}c@{\hskip 15pt}c@{\hskip 15pt}c@{\hskip 15pt}c} 
 \hline
 \textbf{Parameter} & \textbf{$\boldsymbol{\chi^2_{1}(f)}$} & \textbf{$\boldsymbol{\chi^2_{1}(m)}$} & \textbf{$\boldsymbol{\chi^2_2}$} & \textbf{QCDM} & \textbf{Literature} \\ [0.5ex] 
 $L_{\text{peak}}$ (W) & $(3.1\pm0.1)\times 10^{35}$ & $(3.3\pm0.1)\times 10^{35}$ & $(2.94\pm0.04)\times 10^{35}$ & $(3.4\pm0.1 ) \times 10^{35}$ & -\\
 $\Omega_\Lambda$ & $0.86\pm0.04$ &  $0.71\pm0.04$ & $0.74\pm0.01$ & $0.73\pm0.01$ & $0.761^{+0.017}_{-0.018}$ \\
 $\Omega_k$ & - & - & - & $-0.0029\pm0.009$ & $-0.0030^{+0.0095}_{-0.0102}$ \\
 $\Omega_m$ & - & - & - & $0.20\pm0.02$ & $0.239^{+0.018}_{-0.017}$ \\
 $\Omega_r$ & - & - & - & $(4.4\pm0.7)\times10^{-6}$ & $(4.16)\times10^{-6}$ \\
 $w$ & - & - & - & $-0.912\pm0.01$ & $-0.941^{+0.087}_{-0.101}$ \\
 \hline
\end{tabular}
\caption{Cosmological parameters have been calculated using different data sets and methods. $\chi^2_1(f)$ calculated parameters using the least squares method with the SCP and Cal\'{a}n Tololo Survey data in \textit{flux}-space, and $\chi^2_1(m)$ used \textit{magnitude}-space. $\chi^2_2$ are the parameters found with least squares and the larger Union2.1 SCP data set. QCDM corresponds to the parameters calculated using a variant of the Markov Chain Monte Carlo method and we employ the Quintessence Cold Dark Matter model. We additionally provide literature values which have been obtained from M. Tegmark et al (2006) \cite{cosmo_constraints}. We see that as the experiment develops to using different models and analytical methods, the cosmological parameters improve in accuracy.}
\vspace{-1em}
\label{table:cosmo_parameters}
\end{table*}

\begin{figure}[!h]
\begin{center}
\includegraphics[width=9.25cm]{results/hubble_diagram}
\vspace{-3ex}
\caption[]{Hubble diagram showing the Union2.1 Type Ia supernova data plotted alongside two models. The blue line represents a universe with $\Omega_{\Lambda,1}=0.86\pm0.04$ and the red line $\Omega_{\Lambda,2}=0.74\pm0.01$. From reduced $\chi^2$ analysis the former parameter appears to be the best fit with $\chi^2_{\nu,1}=2.4539$ and $\chi^2_{\nu,2}=2.4547$ respectively. We note that there are too many points for the majority of magnitude error bars to be seen.}
\vspace{-3ex}
\label{fig:hubble_diagram}
\end{center}
\end{figure}

In Figure \ref{fig:hubble_diagram} we have plotted the Union2.1 data set alongside two models which make use of the calculated cosmological parameters from flux-space. To assess the quality of fit between the data and the model we have calculated reduced $\chi^2$ values. For the model which makes use of $\Omega_{\Lambda,1}=0.86\pm0.04$ we find $\chi^2_{\nu,1}=2.4539$, and for the secondary model which uses $\Omega_{\Lambda,2}=0.71\pm0.01$ we calculate $\chi^2_{\nu,2}=2.4547$. The criteria for a model to be a good fit to the data is to produce a reduced $\chi^2$ which is $\sim1$ \cite{hugheshase}. We therefore would be justified to conclude that $\Omega_{\Lambda,1}$ should be the accepted cosmological constant as it is a closer value to unity, even if it is not consistent with the literature value.  However, between both $\chi^2_\nu$'s there is only a $0.03\%$ difference and we can see from the figure that they are extremely similar. For that reason we could not form a convincing conclusion based on this analysis, instead we decided to explore Bayesian statistics in an attempt to see if the same results could be replicated and whether the accuracy could be improved further.

\vspace{-3ex}
\subsection{Bayesian Statistics} 
\vspace{-2ex}
With our work in Bayesian statistics we have primarily made use of the Markov Chain Monte Carlo (MCMC) set of methods. Bayes theorem associates the posterior probability (the probability for parameters given known and observed data) to the likelihood (the probability for the data given certain parameters) and the prior (what was known about the parameters before the data). With these statistical procedures we are in essence updating our information about a value from the prior to the posterior \cite{mcmc_bs}. MCMC makes use of this theory and applies it into a methodology, values are generated and then assigned a likelihood probability, criteria is then used to decide if the parameters should be accepted or not, then finally an optimum solution is found from all the completed random walks.

For our work in constraining cosmological parameters we chose to utilise a variant of MCMC, the Random Walk Metropolis-Hastings (RWMH) algorithm. A basic outline of which can be found in Appendix \ref{appendix:mcmc}. We chose to use this as it independently samples the parameter space, information about it's current point does not completely influence the location of it's next `jump' \cite{mcmc_bs}. Instead, an acceptance probability and a random value is compared to generate the next point in the parameter search.

Due to the statistical nature of the MCMC method we were required to perform multiple runs of the same number of walks, and then average the optimum values from each run to find the overall optima. Then we could calculate a standard error which allowed us to more easily compare our results with work from other cosmologists. 

To solely find $\Omega_\Lambda$ we carried out 10 runs of 1500 walks each, from this we found a value of $0.72\pm0.01$. Whilst this does agree with literature, we must admit that the numbers calculated with the least squares method are more accurate. But, this could be the case due to the number of walks given to the algorithm in a single run. If 10,000 walks had been allocated could we have seen the final optimum match more closely to literature? The answer would most probably be no as the search would have been trapped in a `local minima' and it could take many walks before it is able to escape. 

In our early experiments we consistently became confined in these wells, we would find values such as $\Omega_\Lambda=0.47\pm0.02$, which is drastically different to what we have found previously. We can see in Figure \ref{fig:mcmc_search} the dynamics of what occurs. The red dot signifies the initial search parameters for each individual run, for all of our experiments we have allowed this to be $\Omega_\Lambda=0.80$ and $L_\text{peak}=3.40\times10^{35}$ W. With all 10 of our runs we would manage to produce optima which were within the vicinity of $\Omega_\Lambda \approx 0.4$ and $\L_\text{peak}\approx2.9\times10^{35}$ W. This suggests that there was a bias towards these values, we can see that the majority of the generated numbers were within $1$ ands $2$ standard deviations of the optima.

\begin{figure}[!h]
\begin{center}
\includegraphics[width=9.25cm]{results/ol_lp_complete}
\caption[]{Random Walk Metropolis-Hastings algorithm used when constraining $\Omega_\Lambda$ and $L_\text{peak}$. 10 independent runs performed with 1500 individual random walks for each run. The red dot is the initial search parameters of $\Omega_\Lambda=0.8$ and $L_\text{peak}=3.40\times10^{35}$ W, the blue dots represent the optimums for each of the 10 runs, and the green dot is the final average of all of the optimum parameters. The ellipses represent the spread of data with the inner ellipse as $1\sigma$, second as $2\sigma$, and the third as $3\sigma$ - where $\sigma$ is the standard deviation. This plot shows us the potential for the algorithm to become stuck within local minima if left unconstrained. }
\vspace{-5ex}
\label{fig:mcmc_search}
\end{center}
\end{figure}

We could have circumnavigated this problem by implementing \textit{simulated annealing}, where the descent algorithm is modified by adding random ascent moves so local minima could always be escaped \cite{simulated_annealing}. In our experimentation we decided to not implement this, instead we chose to avoid the local minima by limiting the range of numbers that the cosmological values could take - we restricted the available parameter space so it would not become trapped.

\vspace{-3ex}
\subsection{Multi-cosmological Parameters} 
\vspace{-2ex}
We then decided to see what results searching for multiple cosmological parameters would bring. This was an important test as the universe can be described by not just the dark energy density $\Omega_\Lambda$. Therefore we turned to the Lambda Cold Dark Matter ($\Lambda$CDM) model where a proportion of the universe is made up of cold dark matter and vacuum energy \cite{mod_ast, quintessence} and the geometry behaves slightly differently. 

In $\Lambda$CDM the cosmic expansion history $H(z)$ can be given by, 
\begin{multline}
H(z)=H_0[X(z)\Omega_\Lambda+(1+z)^2\Omega_k+\\
(1+z)^3\Omega_m+(1+z)^4\Omega_r]^{1/2},
\label{eqn:expansion_history}
\end{multline}

where $X(z)$ is the dark energy density relative to its present value, $\Omega_k$ is the spatial curvature of the universe, and the other terms are as previously defined in Section \ref{sec:cosmo_parameters} \cite{cosmo_constraints}.

Initially we chose to set $X(z)=1$ as this represents simple `vanilla dark energy' \cite{cosmo_constraints}, later in our research we would find that there could be an alternative model to $\Lambda$CDM where $X(z)$ is not unity. For the case of $X(z)=1$, we believed that a good starting point would be to use the same conditions of 1500 walks for 10 runs and not to limit the search space. Unsurprisingly the search became confined in local minima and the parameters generated were $\sim40\%$ out from literature. To avoid these digressions, we reduced the number of walks to $100$ steps and re-restricted the search space. We chose 100 as from the generated sets we could see that it was around this number that the experiment would deviate and the inaccurate parameters would gain a higher likelihood.

{\renewcommand{\arraystretch}{1.2}%
\begin{table}[h!]
\centering
\begin{tabular}{c@{\hskip 15pt}c@{\hskip 15pt}c} 
 \hline
 \textbf{Parameter} & \textbf{$\boldsymbol{X(z)=1}$} & \textbf{Literature} \\ [0.5ex] 
 $L_{\text{peak}}$ (W) & $(3.3\pm0.1)\times 10^{35}$ & - \\
 $\Omega_\Lambda$ & $0.76\pm0.04$ & $0.761^{+0.017}_{-0.018}$ \\
 $\Omega_k$ & $-0.002\pm0.001$ & $-0.0030^{+0.0095}_{-0.0102}$ \\
 $\Omega_m$ & $0.32\pm0.02$ & $0.239^{+0.018}_{-0.017}$ \\
 $\Omega_r$ & $(3.2\pm0.2)\times10^{-6}$ & $(4.16)\times10^{-6}$ \\
 \hline
\end{tabular}
\caption{We have calculated cosmological parameters using Equation \ref{eqn:expansion_history} with $X(z)=1$. We present the averaged results from our 10 runs of 100 steps each, we limited the search range available for the algorithm to avoid local minima. Literature values have also been provided for comparison. We conclude that we are able to produce consistent values inline with measured literature parameters.}
\vspace{-0.5em}
\label{table:extended_search}
\end{table}

Once confined we could produce results as found in Table \ref{table:extended_search}. Comparisons with the literature values allow us to find that our results are within range and agreement. Using these results as a model, our reduced $\chi^2=2.4489$ which is a small improvement on the least squares method and suggests a better fitting with $\Lambda$CDM. However once we calculated the correlation values between our generated parameters we became skeptical of our work. We know that for some of our variables there should be a relationship, take $\Omega_\Lambda$ and $\Omega_m$, they are related through the total density parameter as given by Equation \ref{eqn:total_density}. Looking at the calculated correlation values in Table \ref{table:correlations} we find that in fact they have no connection, with $-0.105$ to have a correlation the correlation value should be $-1$ or $1$, and $0$ for no relation \cite{hugheshase}. Actually, bar with themselves, there appears to be little to no connection between any of the parameters.

{\renewcommand{\arraystretch}{1.2}%
\begin{table}[h!]
\centering
\begin{tabular}{c@{\hskip 10pt}c@{\hskip 10pt}c@{\hskip 10pt}c@{\hskip 10pt}c@{\hskip 10pt}c} 
 \hline
  & \textbf{$\boldsymbol{L_\text{peak}}$} & \textbf{$\boldsymbol{\Omega_\Lambda}$} & \textbf{$\boldsymbol{\Omega_k}$} & \textbf{$\boldsymbol{\Omega_m}$} & \textbf{$\boldsymbol{\Omega_r}$} \\ [0.5ex] 
 \textbf{$\boldsymbol{L_\text{peak}}$} & +1.000 & $-0.088$ & +0.083 & $-0.213$ & $-0.250$ \\
 \textbf{$\boldsymbol{\Omega_\Lambda}$} & $-0.088$ & +1.000 & $-0.113$ & $-0.105$ & $+0.128$ \\
 \textbf{$\boldsymbol{\Omega_k}$} & +0.083 & $-0.113$ & +1.000 & $-0.107$ & $-0.108$ \\
 \textbf{$\boldsymbol{\Omega_m}$} & $-0.213$ & $-0.105$ & $-0.107$ & +1.000 & +0.004 \\
 \textbf{$\boldsymbol{\Omega_r}$} & $-0.250$ & $+0.128$  & $-0.108$ & +0.004 & +1.000\\
 \hline
\end{tabular}
\caption{Correlation values have been calculated for the $X(z)=1$ model, the cosmological parameters have been taken from Table \ref{table:extended_search}. Between the variables we find little correlation due to the random indepedent generation. We find correlation when the correlation value is $\sim1$ or $\sim -1$, $0$ relates to no correlation \cite{hugheshase}.}
\vspace{-0.5em}
\label{table:correlations}
\end{table}

Whilst our results appear to be in agreement with literature values, with the issue of local minima and with the problem of no correlations, more work has to performed. 

With our multi-parameter testing we then decided to explore the alternative picture where the dark energy density was not constant. We thus moved from the $\Lambda$CDM framework and introduced the concept of \textit{quintessence}. The Quintessence Cold Dark Matter model (QCDM) explains the non-matter component of the total density as a ``mixture of cold dark matter and quintessence'', where quintessence is a slowly-varying and spatially-inhomogeneous component \cite{quintessence}. An equation-of-state $w$ can be attributed to this energy, where the negative pressure $p$ and the density of the energy $\rho$ can be related through $w=p /\rho$. By letting $X(z)=(1+z)^{3(1+w)}$ we can thus modify Equation \ref{eqn:expansion_history} to account for quintessence \cite{cosmo_constraints}. In QCDM we find a potential range of $-1< w \leq 0$, and comparatively $w=-1$ for $\Lambda$CDM \cite{quintessence} which produces the previously defined simplification of $X(z)=1$.

Allowing our algorithm to additionally sample values for $w$ we found that the results for the other parameters seem not to deviate too much from previous results. Viewable in Table \ref{table:cosmo_parameters}, $w$ has just a $3\%$ difference between the calculated number and the literature value. This was a surprising result given that we expected larger deviations to the accuracy given what occurred when we first introduced the $\Lambda$CDM model. 

Nevertheless, once we had our five cosmological parameters from QCDM we used them to generate model magnitudes for supernovae. Pairing this information with the Union2.1 data set we were able to compute $\chi^2_\nu= 2.4166$. We can compare this with our earlier least squares and $\Lambda$CDM work and suggest that there has been an improvement in the fitting potentially due to the change in the model. The inclusion of $w$ amongst the other parameters would now provide a model which is a more accurate representation of the universe, this would in turn provide a better fit to the observational data. 

\vspace{-3ex}
\subsection{Closing Thoughts} 
\vspace{-2ex}
In our work it has been possible to constrain the geometry of the universe through producing model redshifts and magnitudes. However we also had to make use of observational data such as Type Ia supernovae data sets \cite{dataset_1, dataset_2} plus constants such as Hubble's constant \cite{hubble_constant} and the speed of the light \cite{speed_of_light}. The key issue with employing these in our work is the uncertainty in the uncertainties. Whilst the observational data has errors quoted with their main values, more often than not it is difficult to source the origins of how the error was calculated. That is why for us to fully accept our cosmological parameters a further exploration of the systematic uncertainties should be performed.

Once that has been done, different methods of constraining cosmological parameters could then be explored. Other types of observational data could be utilised to provide results which would be independent of Type Ia supernovae. As a verification of our $\Lambda$CDM results, data for the Cosmic Microwave Background (CMB) and for Baryon Acoustic Oscillations (BAO) could be employed to produce cosmic variables, one would have to use a form of the MCMC method to do so \cite{cmb_cosmo, dataset_2}. Then with all three sets of results together: Type Ia supernovae, CMB, and BAO we could then determine with confidence the total density parameter of the universe by superimposing the data and analysing the locations where the data overlaps \cite{dataset_2}.

\vspace{-3ex}
\section{Conclusions}
\vspace{-2ex}
To conclude, we have found that it has been possible to constrain cosmological parameters of the universe using Type Ia supernova data and with statistical methods. We have used a basic least squares method and found that it is able to produce values for $\Omega_\Lambda$ which provide a reasonable fit when a Hubble diagram is plotted, the fitting can then be rated as acceptable with a reduced $\chi^2$ value of 2.4539. For more complex models with multiple parameters we had to employ an MCMC method, with our testing with a quintessence-based QCDM model we managed to produce a model which had $\chi^2_\nu=2.4166$, a slight improvement to the least squares method.

More work definitely has to be done to improve our method for the multi-parameter fitting, in our testing with both $\Lambda$CDM and QCDM models we found that there was always a potential for the searcher algorithm to become trapped within local minima. That is why simulated annealing should be implemented in future work to avoid having to limit the parameter search space to avoid the wells. 

Our experimentation has shown us the practicability for Type Ia supernovae, we have been able to use them to create experimental models which in turn have confirmed observational data. If we are able to reduce the problems of local minima and introduce results which make use of the CMB and BAOs then we would certainly have a powerful way to compute the cosmological parameters of the universe. An important feat if we want to want to understand the geometry and makeup of the universe.

\vspace{-3ex}
\bibliographystyle{unsrt}
\bibliography{supernova_cosmology}

\clearpage
\appendix

\vfill
\twocolumngrid
\vspace{-3ex}
\section{Uncertainties}
\vspace{-2ex}
\subsection{$\chi^2$}
\vspace{-2ex}
In the least squares and RWMH methods we were required to attribute a $\chi^2$ for each model that we produce, this then helped us to decide the optimal cosmological parameters. This was found by using the following equation, 
\begin{equation}
\chi^2 = \sum \frac{f_\text{obs}-f_\text{model}}{\sigma_\text{obs}},
\end{equation}

where $f_\text{obs}$ is the observational flux of the supernovae, $f_\text{model}$ is the model flux, and $\sigma_\text{obs}$ is the uncertainty on the observed flux. This equation was taken from \textit{Measurements and their Uncertainties} [I. G. Hughes and T. P. A. Hase. \textit{Measurements and their Uncertainties}, Oxford University Press, 2010].

\vspace{-3ex}
\subsection{Parameter Uncertainties}
\vspace{-2ex}
Our experimental project made use of observational data which contained errors, in creating our models we produced cosmological parameters which initially had no uncertainty associated with them. We chose to calculate a standard error as this would provide us with an idea of the quality of our data. We collected the optimal parameters for each run and created a final average which in turn allowed us to calculate a standard error. This formula was based off equations from \textit{Measurements and their Uncertainties} [I. G. Hughes and T. P. A. Hase. \textit{Measurements and their Uncertainties}, Oxford University Press, 2010].

The standard error, or standard deviation of the mean can be given by,
\begin{equation}
\alpha=\frac{\sigma_{N-1}}{\sqrt{N}},
\end{equation}

where $\sigma_{N-1}$ is the standard deviation of the average, and $N$ is the number of measurements used.
\clearpage

\vspace{-3ex}
\section{Random Walk Metropolis-Hastings}
\label{appendix:mcmc}
\vspace{-2ex}
To calculate our several cosmological parameters in the $\Lambda$ Cold Dark Matter and Quintessence Cold Dark Matter model we chose to use a form of the Markov Chain Monte Carlo method, we used a Random Walk Metropolis-Hastings algorithm. 

Below, we provide our basic pseudocode used to implement this method within our experimentation. \\

\texttt{def error$\_$function(cosmological$\_$parameters):}
\indent\indent\indent \texttt{""" Function which calculate and } 
\indent\indent\indent \texttt{returns a value for the $\chi^2$ between a}
\indent\indent\indent \texttt{model and experimental data. """} \\

\indent\indent\indent \texttt{return chi$\_$squared} \\

\texttt{def likelihood$\_$ratio(chisq$\_$current, chisq$\_$proposed):} \\
\indent\indent\indent \texttt{""" Function assigns a probability to} 
\indent\indent\indent \texttt{the current $\chi^2$ plus the $\chi^2$ for the} \\
\indent\indent\indent \texttt{randomly generated parameters. """} \\

\indent\indent\indent \texttt{return likelihood} \\

\texttt{def new$\_$values(cosmological$\_$parameters):} \\
\indent\indent\indent \texttt{""" Considers the current inputted} 
\indent\indent\indent \texttt{parameters and generates proposals for} \\
\indent\indent\indent \texttt{the next set of parameters. """} \\

\indent\indent\indent \texttt{new$\_$parameters = randomly generated values from a Gaussian distribution} \\

\indent\indent\indent \texttt{chisq$\_$proposed = uses new$\_$parameters in the error$\_$function to calculate a new chisq} \\

\indent\indent\indent \texttt{ratio = uses likelihood function to provide a probability for the current and proposed parameters together} \\

\indent\indent\indent \texttt{return ratio, new$\_$parameters} \\

\texttt{def ratio$\_$tester(cosmological$\_$parameters):} \\
\indent\indent\indent \texttt{""" Function which performs the random } 
\indent\indent\indent \texttt{walk it takes the new$\_$values functions} \\
\indent\indent\indent \texttt{and samples the parameter space. """} \\

\indent\indent\indent \texttt{loop: \# goes over the entire specified range e.g. 1500 steps} \\
\indent\indent\indent\indent \texttt{> code which inputs the current cosmological parameters into new$\_$values function, it obtains the ratio and then compares it with a randomly generated value between 0 and 1} \\
\indent\indent\indent\indent \texttt{> if statement: if ratio > random value: accept the new$\_$parameters from new$\_$values function and take a walk, otherwise: keep current values and take another walk} \\

\indent\indent\indent \texttt{return data$\_$from$\_$all$\_$runs} \\

\texttt{def optimal$\_$values(data$\_$sample):} \\
\indent\indent\indent \texttt{""" Takes the data from ratio$\_$tester } 
\indent\indent\indent \texttt{and then chooses the optimal parameter} \\
\indent\indent\indent \texttt{by selecting the parameter set which} \\
\indent\indent\indent \texttt{has the greatest likelihood probability. """} \\

\indent\indent\indent \texttt{> code which sorts by likelihood probability and then selects the corresponding parameters} \\

\indent\indent\indent \texttt{return optimal$\_$parameters} \\



\clearpage
\end{document}